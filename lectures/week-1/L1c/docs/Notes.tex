\documentclass{article}[11pt]
\usepackage{fullpage,graphicx, setspace, latexsym, cite,amsmath,amssymb,xcolor,subfigure}
%\usepackage{epstopdf}
%\DeclareGraphicsExtensions{.pdf,.eps,.png,.jpg,.mps} 
\usepackage{amssymb} %maths
\usepackage{amsmath} %maths
\usepackage{amsthm, comment}
\usepackage[round,comma,sort,numbers]{natbib}

% \bibliographystyle{plain}
\bibliographystyle{plos2015}

\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{defn}{Definition}
\newtheorem{ex}{Example}
\usepackage{float}

\newcommand*{\underuparrow}[1]{\underset{\uparrow}{#1}}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[dvipsnames]{xcolor}
\usepackage{algorithmicx}
\usepackage{algorithm} %http://ctan.org/pkg/algorithms
\usepackage{algpseudocode} %http://ctan.org/pkg/algorithmicx
\usepackage{enumitem}
\usepackage{simplemargins}
\usepackage{hyperref}

\usepackage{mdframed}
\definecolor{lgray}{rgb}{0.92,0.92,0.92}
\definecolor{lsalmon}{rgb}{1.0,0.63,0.48}

\renewcommand{\bibnumfmt}[1]{#1.}
\setlist{noitemsep} % or \setlist{noitemsep} to leave space around whole list
\setallmargins{1in}
\linespread{1.1}

\def\R{\mathbb{R}}
\def\Eps{\mathcal{E}}
\def\E{\mathbb{E}}
\def\V{\mathbb{V}}
\def\F{\mathcal{F}}
\def\G{\mathcal{G}}
\def\H{\mathcal{H}}
\def\S{\mathcal{S}}
\def\P{\mathbb{P}}
\def\1{\mathbf{1}}
\def\n{\nappa}
\def\h{\mathbf{w}}
\def\v{\mathbf{v}}
\def\x{\mathbf{x}}
\def\X{\mathcal{X}}
\def\Y{\mathcal{Y}}
\def\eps{\epsilon}
\def\y{\mathbf{y}}
\def\e{\mathbf{e}}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   % \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \setlength{\headsep}{10mm}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CHEME 5820: Machine Learning for Engineers
   \hfill Spring 2025} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}

   \noindent{\bf Disclaimer}: {\it These notes have not been subjected to the
   usual scrutiny reserved for formal publications. }
   \vspace*{4mm}
}

% \mdfdefinestyle{MyFrameStyle}{%
%     linecolor=black,
%     outerlinewidth=2pt,
%     roundcorner=20pt,
%     innertopmargin=-1pt,
%     innerbottommargin=4pt,
%     innerrightmargin=4pt,
%     innerleftmargin=4pt,
%         leftmargin = 4pt,
%         rightmargin = 4pt
%     %backgroundcolor=gray!50!white}
% }


\begin{document}
\lecture{1c}{Unsupervised learning and K-means clustering}{Jeffrey Varner}{}
\begin{mdframed}[backgroundcolor=lgray, innertopmargin=0pt, outerlinewidth=0pt, roundcorner=20pt, innerbottommargin=4pt]
   \section*{Topics}
   \begin{itemize}[leftmargin=16pt]
      \item{\textbf{Unsupervised learning} is a type of machine learning that involves training algorithms on unlabeled data. The goal of unsupervised learning is to identify patterns and structures in data without explicit guidance. 
         Unsupervised learning is particularly useful when dealing with large volumes of unstructured data or when the desired outcomes are unknown.}
      \item{\textbf{Clustering} is a common unsupervised learning technique that involves dividing a dataset into distinct groups, or clusters, based on the similarity of data points. 
         Clustering algorithms aim to group data points that are more similar to each other than to those in other clusters.}
      \item{\textbf{K-means clustering} is a popular and straightforward clustering algorithm that partitions a dataset into $k$ clusters. 
         The algorithm iteratively assigns data points to the nearest cluster center and updates the cluster centers based on the mean of the assigned points.}
   \end{itemize}
\end{mdframed}

\section{Introduction}\label{sec:intro}
This lecture introduces the first of several unsupervised learning approaches we will explore: K-means clustering. 
Unsupervised learning is a branch of machine learning focused on the discovery of hidden patterns and insights from unlabeled data without explicit human guidance. 
Unlike supervised learning, which relies on labeled datasets, unsupervised learning models are given raw, unstructured information and tasked with identifying inherent structures, 
relationships, and similarities within the data. This approach is particularly valuable for exploratory data analysis, clustering similar data points, and uncovering previously unknown trends in datasets. 
The K-means clustering algorithm can be used to identify hidden patterns and structures in data without explicit guidance, or labels.
Thus, it is particularly useful when dealing with large volumes of unstructured data or when the desired outcomes are unknown.
In Chemical and Biomolecular Engineering, K-means clustering can be used to group similar molecules based on their chemical properties \citep{Hadipour:2022aa},
identify patterns in gene expression data \citep{Hruschka2007, Wu:2008aa}, or segment customers based on purchasing behavior \citep{Shaikh2024}.
Thus, there are many applications of K-means clustering in chemical engineering, bioengineering, and related fields.

\section{K-means clustering}
The K-means algorithm, originally developed by Lloyd in the 1950s but not published until much later in 1982 \citep{Lloyd-1982}, is an example of \texttt{unsupervised learning}. 
Unsupervised learning focuses on discovering patterns and structures within data without the guidance of labeled examples or explicit feedback. 
Thus, unlike supervised learning (which we will explore in future lectures), where algorithms are trained on labeled datasets, unsupervised learning algorithms operate with 
raw, unlabeled data to identify inherent groupings, anomalies, or relationships. This approach is particularly valuable when dealing with large volumes of unstructured data or when the desired outcomes may be unknown. 
Typical applications of unsupervised learning include clustering (which we are discussing today), dimensionality reduction, and anomaly detection. 

K-means is a popular unsupervised machine learning algorithm used for clustering data points into $K$ distinct groups based on their similarity.
In this approach, the algorithm partitions the dataset into $K$ (specified by you) clusters, 
with each cluster represented by a centroid (the mean of the data points in the cluster). 
Then the algorithm iteratively assigns data points to the nearest cluster centroid and updates the centroids 
based on the mean of the assigned points.

\subsection{Problem formulation}
Suppose we have a dataset $\mathcal{D} = \{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n\in\mathbb{R}^{m}\}$, where each data point $\mathbf{x}_i$ is a $m$-dimensional vector.
The goal of the k-means algorithm is to partition the dataset into $K$ clusters, $\mathcal{C} = \left\{c_{1},c_{2},\dots, c_{K}\right\}$, 
where each cluster $c_{j}$ is represented by a centroid $\mu_j\in\mathbb{R}^{m}$. To enforce that $\mathcal{C}$ is a valid partition of $\mathcal{D}$, we require that each data point $\mathbf{x}_i$ is assigned to exactly one cluster $c_{j}$, i.e., $c_{i}\cap c_{j} = \emptyset$ for $i\neq j$ (no shared members).
In addition, the union of all clusters covers the entire dataset, i.e., $\cup_{j=1}^{K}c_{j} = \mathcal{D}$.
The problem of k-means clustering can be formulated as an optimization problem, where the objective is to minimize the sum of squared distances between each data point and its assigned cluster centroid:
\begin{equation}
\min_{\mathcal{C}} \sum_{j=1}^{K}\sum_{\mathbf{x}\in c_{j}}\norm{\mathbf{x} - \mu_{j}}_{2}^{2}
\end{equation}
where $\norm{\star}_{2}^{2}$ denotes the squared Euclidean distance between two vectors (in this case a feature vecrtor and the cluster centroid). 
The K-means algorithm aims to find the optimal cluster assignments $\mathcal{C}$ and cluster centroids $\mu$ that minimize this objective function.
Puesdo code for k-means (Lloyd's algorithm) is shown in Algorithm \ref{alg:kmeans}.
\begin{algorithm}[H]
   \begin{algorithmic}
   \caption{Unsupervised naive k-means clustering (Lloyd's algorithm)}\label{alg:kmeans}
   \State \textbf{Input:} $\mathcal{D} = \{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n\in\mathbb{R}^{m}\}$, number of clusters $K$ and initial centroids $\{\mu_1, \mu_2, \ldots, \mu_K\in\mathbb{R}^{m}\}$
   \State \textbf{Output:} Cluster assignments $\mathcal{C} = \{c_1, c_2, \ldots, c_K\}$ and updated cluster centroids $\{\mu_1, \mu_2, \ldots, \mu_K\}$
   \State{$\text{flag} \gets \texttt{false}$}\Comment{flag to indicate convergence: \texttt{true} for convergence and \texttt{false} otherwise}
   \While{$\text{flag}~\text{is}~\texttt{false}$}
   \For{$\mathbf{x}\in \mathcal{D}$}\Comment{Iterate over all data points in $\mathcal{D}$}
      \State{$c_{i}\gets\argmin_{j} \norm{\mathbf{x} - \mu_j}^2$}\Comment{Assign data point $\mathbf{x}$ to the closest cluster centroid (Euclidean distance)}
   \EndFor 
   \Statex
   \State{$\hat{\mu}\gets\mu$}\Comment{Store the current best cluster centroids}
   \For{$j=1$ to $K$}\Comment{Iterate over all clusters}
      \State{$\mu_j\gets{\displaystyle \frac{1}{|c_j|}}\cdot{\displaystyle \sum_{\mathbf{x}\in c_j} \mathbf{x}}$}\Comment{Update cluster centroid $\mu_j$ where $|c_j|$ is the number of data points in cluster $c_j$}
   \EndFor
   \Statex
   \If{$\norm{\mu - \hat{\mu}} < \epsilon$}\Comment{Check for convergence: based on the change in cluster centroids}
      \State{$\text{flag}\gets\texttt{true}$}\Comment{Set flag to \texttt{true} to terminate the algorithm}
   \EndIf
   \EndWhile
   \end{algorithmic}
\end{algorithm}
The time cost of Lloydâ€™s algorithm is $\mathcal{O}(nmk)$, where $n$ is the number of feature vectors, $m$ is the number of dimensions of each feature vector,
and $k$ is the number of clusters.

\section{Estimating the number of clusters}
The K-means algorithm is simple and intuitive, but it has some limitations.
One of the main drawbacks of k-means is that it requires the number of clusters $K$ to be specified in advance, which can be challenging when the number of clusters is unknown.
There are several methods to estimate the number of clusters, including the \href{https://en.wikipedia.org/wiki/Elbow_method_(clustering)}{elbow method}, 
the \href{https://en.wikipedia.org/wiki/Silhouette_(clustering)}{silhouette method}, or performance metrics 
such as the \href{https://en.wikipedia.org/wiki/Davies%E2%80%93Bouldin_index}{Davies-Bouldin index}, the \href{https://en.wikipedia.org/wiki/Dunn_index}{Dunn index}
or the \href{https://en.wikipedia.org/wiki/Calinski%E2%80%93Harabasz_index}{Calinski-Harabasz index}.

\subsection{Silhouette method}
The silhouette method is a technique to evaluate the consistency of data within clusters.
The silhouette score ranges from -1 to 1, where a high score indicates that the data point is well matched to its own cluster and poorly matched to neighboring clusters.
If most objects have a high silhouette score, then the clustering configuration, i.e., the number of clusters $k$ is appropriate. 
However, if many points have a low or negative value, then the clustering configuration may have too many or too few clusters.
Assume that we have clustered the data using k-means into $K$ clusters $\mathcal{C} = \{c_1, c_2, \ldots, c_K\}$.
Then, for a data point $\mathbf{x}_i\in c_i$, let $a(i)$ denote the average distance between $\mathbf{x}_i$ and all other points in the same cluster $c_i$:
\begin{equation}
a(i) = \frac{1}{|c_i| - 1}\sum_{j\in c_i, j\neq i}d(i,j)
\end{equation}
where $|c_i|$ denotes the number of data points in cluster $c_i$, and $d(i,j)$ denotes the distance between data points $\mathbf{x}_i\in{c_{i}}$ and $\mathbf{x}_j\in{c_{i}}$.
Next, we define $b(i)$, the mean dissimilarity of $\mathbf{x}_i$ to all other points not in the same cluster $c_i$:
\begin{equation}
b(i) = \min_{j\neq i}\frac{1}{|c_j|}\sum_{j\in c_j}d(i,j)
\end{equation}
Putting it all together, the silhouette score $s(i)$ for data point $\mathbf{x}_i$ is defined as:
\begin{equation}
s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}\quad\text{if}|c_i|>1
\end{equation}
If $|c_i|=1$, then $s(i)=0$. This gives a silhouette score for each data point as:
\begin{equation}
   s(i) =
   \begin{cases}
      1 - a(i)/b(i) & \text{if } a(i) < b(i)\\ 
      0 & \text{if } a(i) = b(i)\\
      b(i)/a(i) - 1 & \text{if } a(i) > b(i)
   \end{cases}
\end{equation}
From this definition we see that $-1\leq s(i)\leq 1$. 
The mean $s(i)$ over all data points in a cluster is a measure of how tightly grouped all the points in the cluster are.
Thus the mean of $s(i)$ over all data in the entire dataset is a measure of how appropriately the data have been clustered.
If there are too many or too few clusters, as may occur when a poor choice of $K$ is made, then 
some of the clusters will typically display much narrower silhouettes than the rest. 
Thus silhouette plots and means may be used to visualize the natural number of clusters within a dataset.

\subsection{Calinski-Harabasz index}
The \href{https://en.wikipedia.org/wiki/Calinski%E2%80%93Harabasz_index}{Calinskiâ€“Harabasz index (CHI)}, also known as the Variance Ratio Criterion, 
is a widely used metric for assessing the quality of clustering algorithms \citep{Caliski01011974}. To compute the CHI, we first need to perform clustering for different values of k. 
Then, we compute the CH index for each clustering result. Finally, the value of k that yields the maximum CH index is chosen as the optimal number of clusters. The CH index is defined as:
\begin{equation}
\text{CHI} = \left(\frac{n-k}{k-1}\right)\cdot\left(\frac{\sum_{i=1}^{k}n_{i}\lVert{\mathbf{c}_{i}-\mathbf{c}}\rVert^{2}}{\sum_{k=1}^{k}\sum_{\mathbf{x}\in{C_{i}}}\lVert{\mathbf{x}-\mathbf{c}_{i}}\rVert^{2}}\right)
\end{equation}
where $n$ denotes the number of feature vectors $\mathbf{x}_{i}$ that we are clustering, $k$ denotes the number of clusters, $n_{i}$ denotes the number of points in cluster $i$, $\mathbf{c}_{i}$ denotes the centroid for cluster $i$ and $\mathbf{c}$ denotes the overall centroid of the data. 
The numerator measures how well the clusters are separated from each other (the higher, the better), while the denominator measures the compactness or cohesiveness of the clusters (the smaller, the better).


\section{Summary and Conclusion}
In this lecture, we introduced the concept of unsupervised learning and discussed two common unsupervised learning algorithms: k-means clustering and self-organizing maps.
Unsupervised learning is a type of machine learning that involves training algorithms on \textit{unlabeled data} to identify patterns and structures within data without explicit guidance.
Clustering is a common unsupervised learning technique that involves dividing a dataset into distinct groups, or clusters, based on the similarity of data points.
We explored k-means clustering, which partitions a dataset into $k$ clusters and identify patterns that may not be immediately apparent.

\bibliography{References-L1c.bib}

\end{document}
