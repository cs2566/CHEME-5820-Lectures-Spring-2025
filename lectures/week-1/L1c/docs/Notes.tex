 \documentclass{article}[11pt]
\usepackage{fullpage,graphicx, setspace, latexsym, cite,amsmath,amssymb,xcolor,subfigure}
%\usepackage{epstopdf}
%\DeclareGraphicsExtensions{.pdf,.eps,.png,.jpg,.mps} 
\usepackage{amssymb} %maths
\usepackage{amsmath} %maths
\usepackage{amsthm, comment}
\usepackage[round,comma,sort, numbers]{natbib}

% \bibliographystyle{plain}
\bibliographystyle{plos2015}

\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{defn}{Definition}
\newtheorem{ex}{Example}
\usepackage{float}

\newcommand*{\underuparrow}[1]{\underset{\uparrow}{#1}}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[dvipsnames]{xcolor}
\usepackage{algorithmicx}
\usepackage{algorithm} %http://ctan.org/pkg/algorithms
\usepackage{algpseudocode} %http://ctan.org/pkg/algorithmicx
\usepackage{enumitem}
\usepackage{simplemargins}
\usepackage{hyperref}

\renewcommand{\bibnumfmt}[1]{#1.}
\setlist{noitemsep} % or \setlist{noitemsep} to leave space around whole list
\setallmargins{1in}
\linespread{1.1}



\def\R{\mathbb{R}}
\def\Eps{\mathcal{E}}
\def\E{\mathbb{E}}
\def\V{\mathbb{V}}
\def\F{\mathcal{F}}
\def\G{\mathcal{G}}
\def\H{\mathcal{H}}
\def\S{\mathcal{S}}
\def\P{\mathbb{P}}
\def\1{\mathbf{1}}
\def\n{\nappa}
\def\h{\mathbf{w}}
\def\v{\mathbf{v}}
\def\x{\mathbf{x}}
\def\X{\mathcal{X}}
\def\Y{\mathcal{Y}}
\def\eps{\epsilon}
\def\y{\mathbf{y}}
\def\e{\mathbf{e}}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   % \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \setlength{\headsep}{10mm}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CHEME 5820: Machine Learning for Engineers
   \hfill Spring 2025} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}

   \noindent{\bf Disclaimer}: {\it These notes have not been subjected to the
   usual scrutiny reserved for formal publications. }
   \vspace*{4mm}
}


\begin{document}
\lecture{1c}{Unsupervised learning and clustering}{Jeffrey Varner}{}

\section{Introduction}
This lecture introduces the first unsupervised learning approaches we will explore: k-means clustering, and self-organizing maps. 
We will use these algorithms to identify hidden patterns and structures in data without explicit guidance.
The key concepts covered in this lecture include:
\begin{itemize}[leftmargin=16pt]
\item{\textbf{Unsupervised learning} is a type of machine learning that involves training algorithms on unlabeled data. The goal of unsupervised learning is to identify patterns and structures in data without explicit guidance. 
Unsupervised learning is particularly useful when dealing with large volumes of unstructured data or when the desired outcomes are unknown.}
\item{\textbf{Clustering} is a common unsupervised learning technique that involves dividing a dataset into distinct groups, or clusters, based on the similarity of data points. 
Clustering algorithms aim to group data points that are more similar to each other than to those in other clusters.}
\item{\textbf{K-means clustering} is a popular and straightforward clustering algorithm that partitions a dataset into $k$ clusters. 
The algorithm iteratively assigns data points to the nearest cluster center and updates the cluster centers based on the mean of the assigned points.}
\item{\textbf{Self-organizing maps (SOMs)} are another type of unsupervised learning algorithm that uses a neural network to map high-dimensional data onto a lower-dimensional grid.}
\end{itemize}

\section{K-means clustering}
The k-means algorithm, originally developed by Lloyd in the 1950s but not published until much later in 1982 \citep{Lloyd-1982}, is an example of an \texttt{unsupervised learning}. 
Unsupervised learning focuses on discovering patterns and structures in data without the guidance of labeled examples or explicit feedback. 
Unlike supervised learning (which we will explore in future lectures), where algorithms are trained on labeled datasets, unsupervised learning algorithms operate with 
raw, unlabeled data to identify inherent groupings, anomalies, or relationships. This approach is particularly valuable when dealing with large volumes of unstructured data or when the desired outcomes may be unknown. 
Typical applications of unsupervised learning include clustering (which we are discussing today), dimensionality reduction, and anomaly detection. 

K-means is a popular unsupervised machine learning algorithm used for clustering data points into K distinct groups based on their similarity.
In this approach, the algorithm partitions the dataset into $K$ (specified by you) clusters, 
with each cluster represented by a centroid (the mean of the data points in the cluster). 
Then the algorithm iteratively assigns data points to the nearest cluster centroid and updates the centroids based on the mean of the assigned points.
Puesdo code for the k-means algorithm is shown in Algorithm \ref{alg:kmeans}.

\begin{algorithm}[H]
   \begin{algorithmic}[1]
   \caption{Unsupervised naive k-means clustering (Lloyd's algorithm)}\label{alg:kmeans}
   \State \textbf{Input:} Data points $\mathcal{D} = \{x_1, x_2, \ldots, x_n\in\mathbb{R}^{m}\}$, number of clusters $K$
   \State \textbf{Output:} Cluster assignments $C = \{c_1, c_2, \ldots, c_n\}$ and cluster centroids $\{\mu_1, \mu_2, \ldots, \mu_K\}$
   \State Randomly initialize $K$ cluster centroids $\{\mu_1, \mu_2, \ldots, \mu_K\in\mathbb{R}^{m}\}$
   \State{$\text{flag} \gets \texttt{true}$}\Comment{flag to indicate convergence}
   \While{$\text{flag}$}
   \For{$x_{i}\in \mathcal{D}$}
      \State{$c_{i}\gets\underset{j}\argmin \norm{x_{i} - \mu_j}^2$}
   \EndFor 
   \State{$\hat{\mu}\gets\mu$}
   \For{$j=1$ to $K$}
      \State{$\mu_j\gets{\displaystyle \frac{1}{|C_j|}}\cdot{\displaystyle \sum_{x_i\in C_j} x_i}$}
   \EndFor
   \If{$\norm{\mu - \hat{\mu}} < \epsilon$}
      \State{$\text{flag}\gets\texttt{false}$}
   \EndIf
   \EndWhile
   \State \textbf{return} cluster assignments $C$, cluster centroids $\{\mu_1, \mu_2, \ldots, \mu_K\}$
   \end{algorithmic}
\end{algorithm}


\section{Self-organizing maps (SOMs)}
Self-organizing maps (SOMs), originally developed by Kohonen \citep{Kohonen:1982aa}, are another type of unsupervised learning algorithm that uses a graph-like structure to map high-dimensional data onto a lower-dimensional grid.
In the literature, you may also see these referred to as Kohonen maps or topographic maps, or described as a type of artificial neural network
(although they are distinct, and much different in several important ways from traditional neural networks).
SOMs can be used for clustering, visualization, and dimensionality reduction. 
They differ from traditional neural networks in that they use a \texttt{competitive learning} approach 
to map input data to a lower-dimensional grid.

A self-organizing map consists of a rectangular (or potentially hexagonal) grid of nodes organized in a two-dimensional lattice. 
Each node is associated with a weight vector $\mathbf{w}_{j}\in\mathbb{R}^{n}$ which has the same dimension as the input data $\mathbf{x}\in\mathbb{R}^{n}$.
The training of SOMs, which determines the weight vector $\mathbf{w}$, involves two main phases: competition and cooperation.
\begin{itemize}[leftmargin=16pt]
   \item{\textbf{Phase I: Competition}: For each input vector, the node with the weight vector most similar to the input (usually determined by Euclidean distance $||\cdot||_{2}$) is identified as the Best Matching Unit (BMU). 
   This process encourages nodes to compete for representing specific regions of the input space.}
   \item{\textbf{Phase II: Cooperation}: Once the BMU is identified, the weights of the neighboring nodes are updated to become more like the input vector. 
   This is done using a neighborhood function $h:\mathbb{R}\rightarrow\mathbb{R}$ that defines how much influence the BMU 
   has on its neighbors based on their distance from it on the grid.}
\end{itemize}
The weights of the BMU and its neighbors are adjusted at iteration $t$ according to the following expression:
\begin{equation}
   w_{ij}(t+1) = w_{ij}(t) + \alpha(t)h_{ij}(t)(x_i - w_{ij}(t))
\end{equation}
where $w_{ij}(t)$ is the weight of the node at position $(i,j)$ at iteration $t$, 
$\alpha(t)$ is the learning rate at iteration $t$, 
$h_{ij}(t)$ is the neighborhood function at iteration $t$, and $\mathbf{x}_i$ is the input vector.
The neighborhood function $h_{ij}(t)$ is typically a Gaussian function that decreases with the distance from the BMU, i.e., something like:
\begin{equation}
   h_{ij}(t) = \exp\left(-\frac{d_{ij}^2}{2\sigma^2(t)}\right)
\end{equation}
where $d_{ij}$ is the distance between the BMU and the node at position $(i,j)$, and $\sigma(t)$ is the neighborhood radius at iteration $t$.
The learning rate $\alpha(t)$ and the neighborhood radius $\sigma(t)$ are typically annealed over time to allow the network to converge to a stable state.






\section{Summary and Conclusion}
In this lecture, we introduced the concept of unsupervised learning and discussed two common unsupervised learning algorithms: k-means clustering and self-organizing maps.
Unsupervised learning is a type of machine learning that involves training algorithms on \textit{unlabeled data} to identify patterns and structures within data without explicit guidance.
Clustering is a common unsupervised learning technique that involves dividing a dataset into distinct groups, or clusters, based on the similarity of data points.
We explored two clustering algorithms: k-means clustering, which partitions a dataset into $k$ clusters, and self-organizing maps, which use a neural (like) network to map high-dimensional data onto a lower-dimensional grid.
These algorithms can be used to uncover hidden structures in data, visualize complex datasets, and identify patterns that may not be immediately apparent.

\bibliography{References-L1c.bib}

\end{document}
