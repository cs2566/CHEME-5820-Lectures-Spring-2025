\documentclass{article}[11pt]
\usepackage{fullpage,graphicx, setspace, latexsym, cite,amsmath,amssymb,xcolor,subfigure}
%\usepackage{epstopdf}
%\DeclareGraphicsExtensions{.pdf,.eps,.png,.jpg,.mps} 
\usepackage{amssymb} %maths
\usepackage{amsmath} %maths
\usepackage{amsthm, comment}
\usepackage[round,comma,sort, numbers]{natbib}

% \bibliographystyle{plain}
\bibliographystyle{plos2015}

\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{defn}{Definition}
\newtheorem{ex}{Example}
\usepackage{float}

\newcommand*{\underuparrow}[1]{\underset{\uparrow}{#1}}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[dvipsnames]{xcolor}
\usepackage{algorithmicx}
\usepackage{algorithm} %http://ctan.org/pkg/algorithms
\usepackage{algpseudocode} %http://ctan.org/pkg/algorithmicx
\usepackage{enumitem}
\usepackage{simplemargins}
\usepackage{hyperref}

\renewcommand{\bibnumfmt}[1]{#1.}
\setlist{noitemsep} % or \setlist{noitemsep} to leave space around whole list
\setallmargins{1in}
\linespread{1.1}



\def\R{\mathbb{R}}
\def\Eps{\mathcal{E}}
\def\E{\mathbb{E}}
\def\V{\mathbb{V}}
\def\F{\mathcal{F}}
\def\G{\mathcal{G}}
\def\H{\mathcal{H}}
\def\S{\mathcal{S}}
\def\P{\mathbb{P}}
\def\1{\mathbf{1}}
\def\n{\nappa}
\def\h{\mathbf{w}}
\def\v{\mathbf{v}}
\def\x{\mathbf{x}}
\def\X{\mathcal{X}}
\def\Y{\mathcal{Y}}
\def\eps{\epsilon}
\def\y{\mathbf{y}}
\def\e{\mathbf{e}}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   % \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \setlength{\headsep}{10mm}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CHEME 5820: Machine Learning for Engineers
   \hfill Spring 2025} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}

   \noindent{\bf Disclaimer}: {\it These notes have not been subjected to the
   usual scrutiny reserved for formal publications. }
   \vspace*{4mm}
}


\begin{document}
\lecture{1c}{Unsupervised learning and clustering}{Jeffrey Varner}{}

\section{Introduction}
This lecture introduces the first unsupervised learning approaches we will explore: k-means clustering, and self-organizing maps. 
We will use these algorithms to identify hidden patterns and structures in data without explicit guidance.
The key concepts covered in this lecture include:
\begin{itemize}[leftmargin=16pt]
\item{\textbf{Unsupervised learning} is a type of machine learning that involves training algorithms on unlabeled data. The goal of unsupervised learning is to identify patterns and structures in data without explicit guidance. 
Unsupervised learning is particularly useful when dealing with large volumes of unstructured data or when the desired outcomes are unknown.}
\item{\textbf{Clustering} is a common unsupervised learning technique that involves dividing a dataset into distinct groups, or clusters, based on the similarity of data points. 
Clustering algorithms aim to group data points that are more similar to each other than to those in other clusters.}
\item{\textbf{K-means clustering} is a popular and straightforward clustering algorithm that partitions a dataset into $k$ clusters. 
The algorithm iteratively assigns data points to the nearest cluster center and updates the cluster centers based on the mean of the assigned points.}
\end{itemize}

\section{K-means clustering}
The K-means algorithm, originally developed by Lloyd in the 1950s but not published until much later in 1982 \citep{Lloyd-1982}, is an example of an \texttt{unsupervised learning}. 
Unsupervised learning focuses on discovering patterns and structures in data without the guidance of labeled examples or explicit feedback. 
Unlike supervised learning (which we will explore in future lectures), where algorithms are trained on labeled datasets, unsupervised learning algorithms operate with 
raw, unlabeled data to identify inherent groupings, anomalies, or relationships. This approach is particularly valuable when dealing with large volumes of unstructured data or when the desired outcomes may be unknown. 
Typical applications of unsupervised learning include clustering (which we are discussing today), dimensionality reduction, and anomaly detection. 

K-means is a popular unsupervised machine learning algorithm used for clustering data points into $K$ distinct groups based on their similarity.
In this approach, the algorithm partitions the dataset into $K$ (specified by you) clusters, 
with each cluster represented by a centroid (the mean of the data points in the cluster). 
Then the algorithm iteratively assigns data points to the nearest cluster centroid and updates the centroids 
based on the mean of the assigned points.
Puesdo code for the k-means algorithm is shown in Algorithm \ref{alg:kmeans}.

\begin{algorithm}[H]
   \begin{algorithmic}
   \caption{Unsupervised naive k-means clustering (Lloyd's algorithm)}\label{alg:kmeans}
   \State \textbf{Input:} $\mathcal{D} = \{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n\in\mathbb{R}^{m}\}$, number of clusters $K$ and initial centroids $\{\mu_1, \mu_2, \ldots, \mu_K\in\mathbb{R}^{m}\}$
   \State \textbf{Output:} Cluster assignments $\mathcal{C} = \{c_1, c_2, \ldots, c_K\}$ and updated cluster centroids $\{\mu_1, \mu_2, \ldots, \mu_K\}$
   \State{$\text{flag} \gets \texttt{false}$}\Comment{flag to indicate convergence: \texttt{true} for convergence and \texttt{false} otherwise}
   \While{$\text{flag}~\text{is}~\texttt{false}$}
   \For{$\mathbf{x}\in \mathcal{D}$}\Comment{Iterate over all data points in $\mathcal{D}$}
      \State{$c_{i}\gets\argmin_{j} \norm{\mathbf{x} - \mu_j}^2$}\Comment{Assign data point $\mathbf{x}$ to the closest cluster centroid (Euclidean distance)}
   \EndFor 
   \Statex
   \State{$\hat{\mu}\gets\mu$}\Comment{Store the current best cluster centroids}
   \For{$j=1$ to $K$}\Comment{Iterate over all clusters}
      \State{$\mu_j\gets{\displaystyle \frac{1}{|c_j|}}\cdot{\displaystyle \sum_{\mathbf{x}\in c_j} \mathbf{x}}$}\Comment{Update cluster centroid $\mu_j$ where $|c_j|$ is the number of data points in cluster $c_j$}
   \EndFor
   \Statex
   \If{$\norm{\mu - \hat{\mu}} < \epsilon$}\Comment{Check for convergence: based on the change in cluster centroids}
      \State{$\text{flag}\gets\texttt{true}$}\Comment{Set flag to \texttt{true} to terminate the algorithm}
   \EndIf
   \EndWhile
   \end{algorithmic}
\end{algorithm}

\section{Estimating the number of clusters}
The k-means algorithm is simple and intuitive, but it has some limitations.
One of the main drawbacks of k-means is that it requires the number of clusters $K$ to be specified in advance, which can be challenging when the number of clusters is unknown.
There are several methods to estimate the number of clusters, including the \href{https://en.wikipedia.org/wiki/Elbow_method_(clustering)}{elbow method}, 
the \href{https://en.wikipedia.org/wiki/Silhouette_(clustering)}{silhouette method}, or performance metrics 
such as the \href{https://en.wikipedia.org/wiki/Davies%E2%80%93Bouldin_index}{Davies-Bouldin index}, the \href{https://en.wikipedia.org/wiki/Dunn_index}{Dunn index}
or the \href{https://en.wikipedia.org/wiki/Calinski%E2%80%93Harabasz_index}{Calinski-Harabasz index}.

\subsection{Calinski-Harabasz index}
The \href{https://en.wikipedia.org/wiki/Calinski%E2%80%93Harabasz_index}{Calinskiâ€“Harabasz index (CHI)}, also known as the Variance Ratio Criterion, 
is a widely used metric for assessing the quality of clustering algorithms.



\section{Summary and Conclusion}
In this lecture, we introduced the concept of unsupervised learning and discussed two common unsupervised learning algorithms: k-means clustering and self-organizing maps.
Unsupervised learning is a type of machine learning that involves training algorithms on \textit{unlabeled data} to identify patterns and structures within data without explicit guidance.
Clustering is a common unsupervised learning technique that involves dividing a dataset into distinct groups, or clusters, based on the similarity of data points.
We explored k-means clustering, which partitions a dataset into $k$ clusters and identify patterns that may not be immediately apparent.

\bibliography{References-L1c.bib}

\end{document}
