\documentclass{article}[11pt]
\usepackage{fullpage,graphicx, setspace, latexsym, cite,amsmath,amssymb,xcolor,subfigure}
%\usepackage{epstopdf}
%\DeclareGraphicsExtensions{.pdf,.eps,.png,.jpg,.mps} 
\usepackage{amssymb} %maths
\usepackage{amsmath} %maths
\usepackage{amsthm, comment}
\usepackage[round,comma,sort,numbers]{natbib}

% \bibliographystyle{plain}
\bibliographystyle{plos2015}

\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{defn}{Definition}
\newtheorem{ex}{Example}
\usepackage{float}

\newcommand*{\underuparrow}[1]{\underset{\uparrow}{#1}}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[dvipsnames]{xcolor}
\usepackage{algorithmicx}
\usepackage{algorithm} %http://ctan.org/pkg/algorithms
\usepackage{algpseudocode} %http://ctan.org/pkg/algorithmicx
\usepackage{enumitem}
\usepackage{simplemargins}
\usepackage{hyperref}

\usepackage{mdframed}
\definecolor{lgray}{rgb}{0.92,0.92,0.92}
\definecolor{lsalmon}{rgb}{1.0,0.63,0.48}

\renewcommand{\bibnumfmt}[1]{#1.}
\setlist{noitemsep} % or \setlist{noitemsep} to leave space around whole list
\setallmargins{1in}
\linespread{1.1}


\def\R{\mathbb{R}}
\def\Eps{\mathcal{E}}
\def\E{\mathbb{E}}
\def\V{\mathbb{V}}
\def\F{\mathcal{F}}
\def\G{\mathcal{G}}
\def\H{\mathcal{H}}
\def\S{\mathcal{S}}
\def\P{\mathbb{P}}
\def\1{\mathbf{1}}
\def\n{\nappa}
\def\h{\mathbf{w}}
\def\v{\mathbf{v}}
\def\x{\mathbf{x}}
\def\X{\mathcal{X}}
\def\Y{\mathcal{Y}}
\def\eps{\epsilon}
\def\y{\mathbf{y}}
\def\e{\mathbf{e}}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   % \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \setlength{\headsep}{10mm}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CHEME 5820: Machine Learning for Engineers
   \hfill Spring 2025} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}

   \noindent{\bf Disclaimer}: {\it These notes have not been subjected to the
   usual scrutiny reserved for formal publications. }
   \vspace*{4mm}
}


\begin{document}
\lecture{2c}{Singular Value Decomposition (SVD of) Data and Systems}{Jeffrey Varner}{}

\begin{mdframed}[backgroundcolor=lgray]
   \subsection*{Topics}
   \begin{itemize}[leftmargin=16pt]
      \item{\textbf{Singular Value Decomposition (SVD)} is a factorization of a matrix into the product of three matrices: $\mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{T}$, 
      where $\mathbf{U}$ and $\mathbf{V}$ are orthogonal matrices, and $\mathbf{\Sigma}$ is a diagonal matrix containing the singular values of $\mathbf{A}$. The SVD is used to analyze the structure of a matrix, 
      and is used in many applications, including data compression, image processing, and control theory.}
      \item{\textbf{Principle Component Analysis (PCA)} is a method for reducing the dimensionality of data by projecting it onto a lower-dimensional subspace.
      The principal components are the eigenvectors of the covariance matrix of the data, and the principal component scores are the projections of the data onto the principal components.
      We can compute the principal components using the SVD of the covariance matrix.}
   \end{itemize}
\end{mdframed}

\section{Introduction}
In this lecture, we will discuss the singular value decomposition (SVD) of data and systems, and a 
method for reducing the dimensionality of data called principal component analysis (PCA), which is similar to singular value decomposition.
The SVD is a fundamental matrix decomposition that is used in many areas of science and engineering. 
The SVD is a generalization of the eigenvalue decomposition and is used to analyze the structure of a matrix, for non-square matrices.
The SVD is used in a huge variety of unsupervised learning type applications, e.g., understanding gene expression data \citep{Alter:2000aa, Alter:2006}, 
the structure of chemical reaction networks \citep{Famili:2003aa}, in process control applications \citep{MooreSVD1986},
and analyis of various type of networks \citep{SASTRY20075275, 7993780}.
Similarly, PCA is a widely used method across many fields and applicatrions, e.g., drug discovery \citep{GIULIANI20171069}.

\section{Singular Value Decomposition (SVD)}
Singular value decomposition (SVD), originally developed in 1870s \citep{Stewart:1993} is a matrix factorization technique that is based on the eigendecomposition of a matrix.
Suppose we have a matrix $A \in \R^{m \times n}$. The SVD of $\mathbf{A}$ is a factorization of the form: $\mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{T}$, where
$\mathbf{U}\in\mathbb{R}^{n\times{n}}$ and $\mathbf{V}\in\mathbb{R}^{m\times{m}}$ are orthogonal matrices, i.e., $\mathbf{U}\cdot\mathbf{U}^{T} = \mathbf{I}$ and $\mathbf{\Sigma}\in\mathbb{R}^{n\times{m}}$ is a diagonal matrix containing 
the singular values $\sigma_{i}=\Sigma_{ii}$. The matrix $\mathbf{A}\in\mathbb{R}^{n\times{m}}$ can be decomposed as:
\begin{equation*}
\mathbf{A} = \sum_{i=1}^{r_{\mathbf{A}}}\sigma_{i}\cdot\left(\mathbf{u}_{i}\otimes\mathbf{v}_{i}\right)
\end{equation*}
where $r_{\mathbf{A}}$ is the rank of matrix $\mathbf{A}$ and $\otimes$ denotes the outer product. 
The outer product $\hat{\mathbf{A}}_{i} = \mathbf{u}_{i}\otimes\mathbf{v}_{i}$ is a rank-1 matrix with elements: 
\begin{equation*}
\hat{a}_{jk} = u_{j}v_{k}\qquad{j=1,2,\dots,n~\text{and}~k=1,2,\dots,m}
\end{equation*}
The vectors $\mathbf{u}_{i}$ and $\mathbf{v}_{i}$ are the left (right) singular vectors, 
and $\sigma_{i}$ are the singular values (ordered). Singular value decomposuition is a special sort of eigendecomposition, thus, we could use QR-itereation to compute the SVD.
The columns of $\mathbf{U}$ are eigenvectors of $\mathbf{A}\mathbf{A}^{T}$, 
the columns of $\mathbf{V}$ are eigenvectors of $\mathbf{A}^{T}\mathbf{A}$ and
the singular values $\sigma_{i}$ are the square roots of the eigenvalues of the matrix products $\mathbf{A}\mathbf{A}^{T}$ or $\mathbf{A}^{T}\mathbf{A}$. 

\section{Principal Component Analysis (PCA)}
Principal component analysis (PCA) is a method for reducing the dimensionality of data by projecting it onto a lower-dimensional subspace.
The principal components are the eigenvectors of the covariance matrix of the data, and the principal component scores are the projections of the data onto the principal components.
Let's return to the idea of dimensionality reduction that we introduced when exploring eigendecomposition.

\section{Summary and Conclusions}
In this lecture we discussed the singular value decomposition (SVD) of data and systems, and a method for reducing the dimensionality of data called principal component analysis (PCA).
Singular Value Decomposition (SVD) and Principal Component Analysis (PCA) are powerful techniques in linear algebra and data analysis. 
SVD is a matrix factorization method that decomposes a matrix $\mathbf{A}$ into the product $\mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{T}$, where
$\mathbf{U}\in\mathbb{R}^{n\times{n}}$ and $\mathbf{V}\in\mathbb{R}^{m\times{m}}$ are orthogonal matrices, i.e., $\mathbf{U}\cdot\mathbf{U}^{T} = \mathbf{I}$ and $\mathbf{\Sigma}\in\mathbb{R}^{n\times{m}}$ 
is a diagonal matrix containing the singular values $\sigma_{i}=\Sigma_{ii}$.
This decomposition has important applications in dimensionality reduction and data compression. 
On the other hand, principle component analysis (PCA), closely related to SVD, 
is a method for reducing the dimensionality of multivariate data while preserving as much variance as possible (or alternatively minimize the reconstruction error). 
It works by identifying principal components, which are orthogonal directions in the data space that capture the most variation (or minimize the construction error). 
Both SVD and PCA are widely used in various fields, including machine learning, image processing, and gene expression analysis, 
for tasks such as feature extraction, noise reduction, and data visualization.

\bibliography{References-L2c.bib}

\end{document}


