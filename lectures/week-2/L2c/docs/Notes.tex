\documentclass{article}[11pt]
\usepackage{fullpage,graphicx, setspace, latexsym, cite,amsmath,amssymb,xcolor,subfigure}
%\usepackage{epstopdf}
%\DeclareGraphicsExtensions{.pdf,.eps,.png,.jpg,.mps} 
\usepackage{amssymb} %maths
\usepackage{amsmath} %maths
\usepackage{amsthm, comment}
\usepackage[round,comma,sort,numbers]{natbib}

% \bibliographystyle{plain}
\bibliographystyle{plos2015}

\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{defn}{Definition}
\newtheorem{ex}{Example}
\usepackage{float}

\newcommand*{\underuparrow}[1]{\underset{\uparrow}{#1}}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[dvipsnames]{xcolor}
\usepackage{algorithmicx}
\usepackage{algorithm} %http://ctan.org/pkg/algorithms
\usepackage{algpseudocode} %http://ctan.org/pkg/algorithmicx
\usepackage{enumitem}
\usepackage{simplemargins}
\usepackage{hyperref}

\usepackage{mdframed}
\definecolor{lgray}{rgb}{0.92,0.92,0.92}
\definecolor{lsalmon}{rgb}{0.9921568627450981,0.9411764705882353, 0.9254901960784314}

\renewcommand{\bibnumfmt}[1]{#1.}
\setlist{noitemsep} % or \setlist{noitemsep} to leave space around whole list
\setallmargins{1in}
\linespread{1.1}

\newcommand{\brows}[1]{%
  \begin{bmatrix}
  \begin{array}{@{\protect\rotvert\;}c@{\;\protect\rotvert}}
  #1
  \end{array}
  \end{bmatrix}
}
\newcommand{\rotvert}{\rotatebox[origin=c]{90}{$\vert$}}
\newcommand{\rowsvdots}{\multicolumn{1}{@{}c@{}}{\vdots}}


\def\R{\mathbb{R}}
\def\Eps{\mathcal{E}}
\def\E{\mathbb{E}}
\def\V{\mathbb{V}}
\def\F{\mathcal{F}}
\def\G{\mathcal{G}}
\def\H{\mathcal{H}}
\def\S{\mathcal{S}}
\def\D{\mathcal{D}}
\def\P{\mathbb{P}}
\def\1{\mathbf{1}}
\def\n{\nappa}
\def\h{\mathbf{w}}
\def\v{\mathbf{v}}
\def\x{\mathbf{x}}
\def\X{\mathcal{X}}
\def\Y{\mathcal{Y}}
\def\eps{\epsilon}
\def\y{\mathbf{y}}
\def\e{\mathbf{e}}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   % \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \setlength{\headsep}{10mm}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CHEME 5820: Machine Learning for Engineers
   \hfill Spring 2025} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}

   \noindent{\bf Disclaimer}: {\it These notes have not been subjected to the
   usual scrutiny reserved for formal publications. }
   \vspace*{4mm}
}

\begin{document}
\lecture{2c}{Singular Value Decomposition (SVD of) Data and Systems}{Jeffrey Varner}{}

\begin{mdframed}[backgroundcolor=lgray]
   \subsection*{Topics}
   \begin{itemize}[leftmargin=16pt]
      \item{\textbf{Singular Value Decomposition (SVD)} is a factorization of a matrix into the product of three matrices: $\mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{T}$, 
      where $\mathbf{U}$ and $\mathbf{V}$ are orthogonal matrices, and $\mathbf{\Sigma}$ is a diagonal matrix containing the singular values of $\mathbf{A}$. The SVD is used to analyze the structure of a matrix, 
      and is used in many applications, including data compression, image processing, and control theory.}
      \item{\textbf{Principle Component Analysis (PCA)} is a method for reducing the dimensionality of data by projecting it onto a lower-dimensional subspace.
      The principal components are the eigenvectors of the covariance matrix of the data, and the principal component scores are the projections of the data onto the principal components.
      We can compute the principal components using the SVD of the covariance matrix.}
   \end{itemize}
\end{mdframed}

\section{Introduction}
In this lecture, we will discuss the singular value decomposition (SVD) of data and systems, and a 
method for reducing the dimensionality of data called principal component analysis (PCA), which is similar to singular value decomposition.
The SVD is a fundamental matrix decomposition that is used in many areas of science and engineering. 
The SVD is a generalization of the eigenvalue decomposition and is used to analyze the structure of a matrix, for non-square matrices.
The SVD is used in a huge variety of unsupervised learning type applications, e.g., understanding gene expression data \citep{Alter:2000aa, Alter:2006}, 
the structure of chemical reaction networks \citep{Famili:2003aa}, in process control applications \citep{MooreSVD1986},
and analyis of various type of networks \citep{SASTRY20075275, 7993780}.
Similarly, PCA is a widely used method across many fields and applicatrions, e.g., drug discovery \citep{GIULIANI20171069}.

\section{Singular Value Decomposition (SVD)}
Singular value decomposition (SVD), originally developed in 1870s \citep{Stewart:1993} is a matrix factorization technique that is based on the eigendecomposition of a matrix.
Suppose we have a matrix $A \in \R^{m \times n}$. The SVD of $\mathbf{A}$ is a factorization of the form: $\mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{T}$, where
$\mathbf{U}\in\mathbb{R}^{n\times{n}}$ and $\mathbf{V}\in\mathbb{R}^{m\times{m}}$ are orthogonal matrices, i.e., $\mathbf{U}\cdot\mathbf{U}^{T} = \mathbf{I}$ and $\mathbf{\Sigma}\in\mathbb{R}^{n\times{m}}$ is a diagonal matrix containing 
the singular values $\sigma_{i}=\Sigma_{ii}$. The matrix $\mathbf{A}\in\mathbb{R}^{n\times{m}}$ can be decomposed as:
\begin{equation*}
\mathbf{A} = \sum_{i=1}^{r_{\mathbf{A}}}\sigma_{i}\cdot\left(\mathbf{u}_{i}\otimes\mathbf{v}_{i}\right)
\end{equation*}
where $r_{\mathbf{A}}$ is the rank of matrix $\mathbf{A}$ and $\otimes$ denotes the outer product. 
The outer product $\hat{\mathbf{A}}_{i} = \mathbf{u}_{i}\otimes\mathbf{v}_{i}$ is a rank-1 matrix with elements: 
\begin{equation*}
\hat{a}_{jk} = u_{j}v_{k}\qquad{j=1,2,\dots,n~\text{and}~k=1,2,\dots,m}
\end{equation*}
The vectors $\mathbf{u}_{i}$ and $\mathbf{v}_{i}$ are the left (right) singular vectors, 
and $\sigma_{i}$ are the singular values (ordered). Singular value decomposuition is a special sort of eigendecomposition, thus, we could use QR-itereation to compute the SVD.
The columns of $\mathbf{U}$ are eigenvectors of $\mathbf{A}\mathbf{A}^{T}$, 
the columns of $\mathbf{V}$ are eigenvectors of $\mathbf{A}^{T}\mathbf{A}$ and
the singular values $\sigma_{i}$ are the square roots of the eigenvalues of the matrix products $\mathbf{A}\mathbf{A}^{T}$ or $\mathbf{A}^{T}\mathbf{A}$. 

\section{Principal Component Analysis (PCA)}
Principal component analysis (PCA) is a method for reducing the dimensionality of data by projecting it onto a lower-dimensional subspace.
The principal components are the eigenvectors of the covariance matrix of the data, and the principal component scores are the projections of the data onto the principal components.

Let's return to the idea of dimensionality reduction of the covariance matrix that we introduced when exploring eigendecomposition.
Suppose we have a dataset $\D = \left\{\mathbf{x}_{1},\mathbf{x}_{2},\dots,\mathbf{x}_{n}\right\}$ where each $\mathbf{x}_{i}\in\mathbb{R}^{m}$ is an $m$-dimensional feature vector.
The covariance of feature vectors $i$ and $j$, denoted as $\text{cov}\left(\mathbf{x}_{i},\mathbf{x}_{j}\right)$, is an $\mathbf{\Sigma}\in\mathbb{R}^{n\times{n}}$ 
real-valued symmetric matrix $\mathbf{\Sigma}\in\R^{n\times{n}}$ with elements: 
\begin{equation}
    \Sigma_{ij} = \text{cov}\left(\mathbf{x}_{i},\mathbf{x}_{j}\right) = \sigma_{i}\,\sigma_{j}\,\rho_{ij}\qquad\text{for}\quad{i,j \in \mathcal{D}}
\end{equation}
where $\sigma_{i}$ denote the standard deviation of the feature vector $\mathbf{x}_{i}$, $\sigma_{j}$ denote the standard deviation of the 
feature vector $\mathbf{x}_{j}$, and $\rho_{ij}$ denotes the correlation between features $i$ and $j$ in the dataset $\D$. The correlation is given by:
\begin{equation}
\rho_{ij} = \frac{\mathbb{E}(\mathbf{x}_{i}-\mu_{i})\cdot\mathbb{E}(\mathbf{x}_{j} - \mu_{j})}{\sigma_{i}\sigma_{j}}\qquad\text{for}\quad{i,j \in \mathcal{D}}
\end{equation}
The diagonal elements of the covariance matrix $\Sigma_{ii}\in\mathbf{\Sigma}$ are the variances of the individual feature vectors,
while the off-diagonal elements $\Sigma_{ij}\in\mathbf{\Sigma}$ for $i\neq{j}$ measure the relationship between feature vectors $i$ and $j$
in the dataset $\mathcal{D}$. 

\subsection{Data reduction problem}
Suppose we wanted to reduce the dimensionality of the feature vectors $\mathbf{x}\in\D$ from $m$ to $k$ dimensions, where $k<m$, 
i.e., we want to transform $\mathbf{x}_{i}\in\R^{m}\rightarrow\mathbf{y}_{i}\in\R^{k}$. 
We may want to do this for a variety of reasons, for example, to visualize the data in $2$ or $3$ dimensions, or to reduce the computational complexity of a machine learning algorithm.
To make this possible, we construct (somehow) a projection matrix $\mathbf{P}$ such that:
\begin{equation}
   \mathbf{y}_{i} = \mathbf{P}\mathbf{x}_{i}\quad{i=1,2,\dots,n}
\end{equation}
The projection matrix $\mathbf{P}$ will be a $k\times{m}$ matrix composed of some transform vectors. The open question is 
what are the best transform vectors to use? The answer is to use the eigenvectors of the covariance matrix $\mathbf{\Sigma}$.
However, we build our transformation matrix using the eigenvectors gives the reduced feature vector corresponding to $x\in\D$ is given by:
\begin{equation}
   \begin{pmatrix}
      y_{1} \\
      y_{2} \\
      \vdots \\
      y_{k}
   \end{pmatrix} = \brows{\hat{\mathbf{v}}_1^T \\ \hat{\mathbf{v}}_2^T \\ \rowsvdots \\ \hat{\mathbf{v}}_k^T}
   \cdot
   \begin{pmatrix}
      x_{1} \\
      x_{2} \\
      \vdots \\
      x_{m}
   \end{pmatrix}
\end{equation}
where $\hat{\mathbf{v}}_{\star}^{T}$ denotes the transpose of the scaled eigenvector $\hat{\mathbf{v}}_{\star}$ of the covariance matrix $\mathbf{\Sigma}$.
Thus, each component of the reduced vector $\mathbf{y}$ corresponding to $\mathbf{x}\in\D$ is given by:
\begin{equation}
   y_{j} = \hat{\mathbf{v}}_{j}^{T}\cdot\mathbf{x}\quad{j=1,2,\dots,k}
\end{equation}
The open question is which $k$-vectors should we use to build the transformation matrix $\mathbf{P}$? 

\subsection*{Minimize reconstruction error}
Fill me in.

\section{Summary and Conclusions}
In this lecture we discussed the singular value decomposition (SVD) of data and systems, and a method for reducing the dimensionality of data called principal component analysis (PCA).
Singular Value Decomposition (SVD) and Principal Component Analysis (PCA) are powerful techniques in linear algebra and data analysis. 
SVD is a matrix factorization method that decomposes a matrix $\mathbf{A}$ into the product $\mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{T}$, where
$\mathbf{U}\in\mathbb{R}^{n\times{n}}$ and $\mathbf{V}\in\mathbb{R}^{m\times{m}}$ are orthogonal matrices, i.e., $\mathbf{U}\cdot\mathbf{U}^{T} = \mathbf{I}$ and $\mathbf{\Sigma}\in\mathbb{R}^{n\times{m}}$ 
is a diagonal matrix containing the singular values $\sigma_{i}=\Sigma_{ii}$.
This decomposition has important applications in dimensionality reduction and data compression. 
On the other hand, principle component analysis (PCA), closely related to SVD, 
is a method for reducing the dimensionality of multivariate data while preserving as much variance as possible (or alternatively minimize the reconstruction error). 
It works by identifying principal components, which are orthogonal directions in the data space that capture the most variation (or minimize the construction error). 
Both SVD and PCA are widely used in various fields, including machine learning, image processing, and gene expression analysis, 
for tasks such as feature extraction, noise reduction, and data visualization.

\bibliography{References-L2c.bib}

\end{document}


