{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d0c28b-498e-4e20-a8ea-8ea5a5aecbac",
   "metadata": {},
   "source": [
    "# Lecture 2a: Eigendecomposition of Data and Systems\n",
    "In this lecture, we will discuss the eigendecomposition of a matrix and how it can be used to analyze data and systems in unsupervised machine learning. \n",
    "\n",
    "#### Key ideas in this lecture\n",
    "* __Eigendecomposition__ allows us to decompose a matrix into its constituent parts, the eigenvectors and eigenvalues. These values can help us understand the structure of the data or system represented by the matrix. Several approaches can be used to estimate the eigenvalues and eigenvectors of a matrix.\n",
    "* __Power iteration method__: given a diagonalizable matrix $\\mathbf{A}$ the algorithm will produce a number $\\lambda$, which is the greatest (in absolute value) eigenvalue of $\\mathbf{A}$ and a nonzero vector $\\mathbf{v}$ which is a corresponding eigenvector of $\\lambda$ such that $\\mathbf{A}\\mathbf{v} = \\lambda\\cdot\\mathbf{v}$.\n",
    "* __QR factorization__ is another approach to compute the eigendecomposition of the matrix $\\mathbf{A}$. However, unlike power iteration, this approach will give all eigenvalues and eigenvectors of the matrix. The QR factorization algorithm relies on the [QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition), which itself relies on the [Gram-Schmidt algorithm](https://en.wikipedia.org/wiki/Gramâ€“Schmidt_process)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a344371-a7f4-4377-aada-441a1aa149d1",
   "metadata": {},
   "source": [
    "## Eigendecomposition\n",
    "Suppose we have a real square matrix $\\mathbf{A}\\in\\mathbb{R}^{m\\times{m}}$ which could be a measurement dataset, e.g., the columns of $\\mathbf{A}$ represent feature \n",
    "vectors $\\mathbf{x}_{1},\\dots,\\mathbf{x}_{m}$ or an incidence array in a graph, etc. Eigenvalue-eigenvector problems involve finding a set of scalar values $\\left\\{\\lambda_{1},\\dots,\\lambda_{m}\\right\\}$ called \n",
    "[eigenvalues](https://mathworld.wolfram.com/Eigenvalue.html) and a set of linearly independent vectors \n",
    "$\\left\\{\\mathbf{v}_{1},\\dots,\\mathbf{v}_{m}\\right\\}$ called [eigenvectors](https://mathworld.wolfram.com/Eigenvector.html) such that:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{A}\\mathbf{v}_{j} = \\lambda_{j}\\cdot\\mathbf{v}_{j}\\qquad{j=1,2,\\dots,m}\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\mathbf{v}\\in\\mathbb{R}^{m}$ and $\\lambda\\in\\mathbb{R}$. \n",
    "\n",
    "### Why is this interesting?\n",
    "Eigenvectors represent fundamental directions of the matrix $\\mathbf{A}$. For the linear transformation defined by a matrix $\\mathbf{A}$, the eigenvectors are the only vectors that do not change direction during the transformation. Instead, they are scaled by a factor $\\lambda$ called the eigenvalue. Thus, eigenvectors represent the most critical directions in the data or system, and the eigenvalues represent the importance of these directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f68694-55fd-4352-82e4-9956f7b48855",
   "metadata": {},
   "source": [
    "## Power iteration\n",
    "Fill me in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4080a7-b124-4045-801d-121835f0042f",
   "metadata": {},
   "source": [
    "## QR factorization and Gram-Schmidt\n",
    "Fill me in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7996efd3-fe2a-4aa4-a9a3-f7469c0d1338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
