{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d0c28b-498e-4e20-a8ea-8ea5a5aecbac",
   "metadata": {},
   "source": [
    "# Lecture 2a: Eigendecomposition of Data and Systems\n",
    "In this lecture, we will discuss the eigendecomposition of a square matrix and how it can be used to understand data and systems in unsupervised machine learning. There are several key ideas in this lecture:\n",
    "\n",
    "* __Eigendecomposition__ allows us to decompose a matrix into its constituent parts, the [eigenvectors and eigenvalues](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors). These values help us understand the structure of the data or system represented by the matrix. We'll look at two approaches to estimate the [eigenvalues and eigenvectors](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors) of a matrix:\n",
    "    * __Power iteration method__ estimates the _largest_ eigenvalue/eigenvector pair. Given a _diagonalizable_ matrix $\\mathbf{A}$ the power iteration algorithm will produce a number $\\lambda$, which is the greatest (in absolute value) eigenvalue of $\\mathbf{A}$ and a nonzero vector $\\mathbf{v}$ which is a corresponding eigenvector of $\\lambda$ such that $\\mathbf{A}\\mathbf{v} = \\lambda\\cdot\\mathbf{v}$.\n",
    "    * __QR iteration__ is another approach to compute the eigendecomposition of the matrix $\\mathbf{A}$. However, unlike power iteration, this approach will give all eigenvalues and eigenvectors of the matrix $\\mathbf{A}$. The QR factorization algorithm relies on the [QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition), which itself relies on the [Gram-Schmidt algorithm](https://en.wikipedia.org/wiki/Gram–Schmidt_process).\n",
    "* __Buy versus build__: While we will explore these two approaches to compute the eigendecomposition of a matrix in the lecture and associated lab, most, if not all, computing platforms already have built-in functionality to do this computation. For example, [Julia has the `eigen(...)` function exported by the LinearAlgebra.jl package](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigen). Most of the time, there is no need to recreate the wheel (and your implementation will likely be worse in space and time complexity). So always buy when given the chance if the buy option is good.\n",
    "\n",
    "Lecture notes can be found: [here!](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-2/L2a/docs/Notes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9085c0c-4cbd-4e22-9b47-1127307d413f",
   "metadata": {},
   "source": [
    "## Setup, Data and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. The `Include.jl` file loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "dc0dc6e0-1af4-4406-af0f-67e920dac536",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde34514-8a96-4197-80e3-8aaa98834ab7",
   "metadata": {},
   "source": [
    "### Data\n",
    "We'll use the coagulation dataset during pregnancy. Let's load this data from disk using [the `MySyntheticDataSet()` function](src/Files.jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "afab24e4-49f5-4235-bad3-014cfe722e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MySyntheticDataset() |> d-> d[\"ensemble\"]; # this is loading visit 1 data (baseline, non-pregnant) by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4b5d4612-a983-41c4-965a-d47fbe0ed083",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_patient_index = 7; # i ∈ {keys}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2e02d2-4996-4279-acd9-ff1c2c99c622",
   "metadata": {},
   "source": [
    "The keys of the dataset dictionary are the `actual` patient indexes. These keys point to `synthetic` patient measurement vectors constructed by building a model of the original data distribution. To explore this data, specify an original patient index (one of the keys of the original dictionary) in the `original_patient_index::Int` variable:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ef3440-055c-4a50-b5bd-d4f9eab9f87a",
   "metadata": {},
   "source": [
    "Next, we'll build a data matrix with the `synthetic` measurement vectors for the specified original patient index. We'll store this in the `D::Array{<:Number, 1}` matrix. We'll have the option to [z-score center each feature of the dataset](https://en.wikipedia.org/wiki/Feature_scaling).\n",
    "* __Scaling__: In [z-score feature scaling](https://en.wikipedia.org/wiki/Feature_scaling), we subtract off the mean of each feature and then divide by the standard deviation, i.e., $x^{\\prime} = (x - \\mu)/\\sigma$ where $x$ is the unscaled data, and $x^{\\prime}$ is the scaled data. Under this scaling regime, $x^{\\prime}\\leq{0}$ will be values that are less than or equal to the mean value $\\mu$, while $x^{\\prime}>0$ indicate values that are greater than the mean. The range of data is measured in quanta of the standard deviation $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0eb22fb3-5846-48a1-b8fe-2330a8d921e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = let\n",
    "\n",
    "    M = dataset[original_patient_index];\n",
    "    zscore_convert_data_flag = true; # if this is true, we zscore center the data\n",
    "    number_of_rows = length(M); # number of synthetic patients\n",
    "    number_of_cols = length(M[1]) - 1; # number of measurements (features), first col is the visit number\n",
    "    D = Array{Float64,2}(undef, number_of_rows, number_of_cols);\n",
    "\n",
    "    for i ∈ 0:(number_of_rows - 1)\n",
    "        for j ∈ 1:(number_of_cols)\n",
    "            D[i+1,j] = M[i][j+1];\n",
    "        end\n",
    "    end\n",
    "\n",
    "    D̂ = copy(D); # z-scale this data? \n",
    "    if (zscore_convert_data_flag == true) \n",
    "        for j ∈ 1:number_of_cols\n",
    "            sample_vector = D[:,j]; \n",
    "            μ = mean(sample_vector);\n",
    "            σ = std(sample_vector);\n",
    "\n",
    "            for i ∈ 1:number_of_rows\n",
    "                D̂[i,j] = (sample_vector[i] - μ)/σ;\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    D̂ # return\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a1a1c505-1f40-4c2f-b066-7c0729fd7e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101×32 Matrix{Float64}:\n",
       " -0.235898    -0.400982    0.491598   …  -0.945111   -0.240582   -1.28826\n",
       "  2.64205     -0.112588    0.604192       1.06829     0.292611   -0.31851\n",
       " -0.426973    -0.125557    0.727522      -2.36602     0.937498   -0.430226\n",
       " -0.00917291   1.07028    -0.274728       0.748622   -0.04306     0.765123\n",
       "  2.39505      1.179       1.16139        0.546061   -0.49863     0.337057\n",
       "  0.455682    -0.612293   -0.323544   …  -0.499281   -1.41524     1.24904\n",
       "  0.00108153   0.0340816   1.52975       -1.00115    -0.0296104   2.00794\n",
       " -0.1397      -0.0683655   0.71844        0.124909    1.5044     -0.218094\n",
       "  0.0412372    1.65581    -0.657545       1.89657    -0.158904   -0.61306\n",
       " -0.659116    -1.29706     1.5989        -0.667092    0.840188   -0.839817\n",
       " -1.20167     -0.991119    0.111004   …   0.938794    0.0608561   0.205705\n",
       "  1.51375      2.73005     1.82544       -1.10368    -0.555378    1.44378\n",
       " -0.386925    -0.0698584  -0.31693       -1.3328     -0.491284    2.64475\n",
       "  ⋮                                   ⋱               ⋮          \n",
       " -0.881309    -0.147794   -0.0781565      0.45916     0.064316   -0.597477\n",
       " -0.727187    -0.580028   -0.258569   …   0.388513    1.00956     1.29238\n",
       "  0.00350472  -0.692023    0.293364      -0.941494    1.39037     0.761357\n",
       "  2.69785     -0.376613   -0.663341      -0.651085   -1.01849    -0.348566\n",
       " -0.649242    -0.521492    0.357641       2.27465     0.342584   -0.247955\n",
       " -1.34332     -0.664685    1.42886       -1.39971     0.793604   -0.863356\n",
       " -0.0778424   -0.690185    0.47985    …  -0.0807288   0.0490697  -1.2778\n",
       "  0.149885    -0.303918   -0.257433       1.36215    -0.889817    0.0124175\n",
       "  0.469944    -1.03231     0.589305      -0.398318    0.433392    0.952518\n",
       "  1.30708     -0.562379   -0.632626       0.533305    1.37282    -0.461387\n",
       " -0.623295     0.908115   -1.76302       -1.69344    -0.943364   -2.36715\n",
       "  0.238155     1.16029    -0.342527   …   0.271978   -1.12804    -0.475899"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f18c2a0-38e0-4b40-a8e1-417810710637",
   "metadata": {},
   "source": [
    "### Covariance matrix\n",
    "Next, compute the covariance matrix $\\mathbf{\\Sigma}$, a square symmetric matrix whose eigenvalues have some magical properties. The covariance matrix is a square matrix that summarizes the variance and covariance of the features in the dataset.\n",
    "Suppose we have a dataset $\\mathcal{D} = \\left\\{\\mathbf{x}_{1},\\mathbf{x}_{2},\\dots,\\mathbf{x}_{n}\\right\\}$ where each $\\mathbf{x}_{i}\\in\\mathbb{R}^{m}$ is a feature vector.\n",
    "The covariance of feature vectors $i$ and $j$, denoted as $\\text{cov}\\left(\\mathbf{x}_{i},\\mathbf{x}_{j}\\right)$, is a real-valued symmetric matrix $\\mathbf{\\Sigma}\\in\\mathbb{R}^{n\\times{n}}$ with elements: \n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\Sigma_{ij} = \\text{cov}\\left(\\mathbf{x}_{i},\\mathbf{x}_{j}\\right) = \\sigma_{i}\\,\\sigma_{j}\\,\\rho_{ij}\\qquad\\text{for}\\quad{i,j \\in \\mathcal{D}}\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\sigma_{i}$ denote the standard deviation of the feature vector $\\mathbf{x}_{i}$, $\\sigma_{j}$ denote the standard deviation of the \n",
    "feature vector $\\mathbf{x}_{j}$, and $\\rho_{ij}$ denotes the correlation between features $i$ and $j$ in the dataset $\\mathcal{D}$. The correlation is given by:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\rho_{ij} = \\frac{\\mathbb{E}(\\mathbf{x}_{i}-\\mu_{i})\\cdot\\mathbb{E}(\\mathbf{x}_{j} - \\mu_{j})}{\\sigma_{i}\\sigma_{j}}\\qquad\\text{for}\\quad{i,j \\in \\mathcal{D}}\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\mathbb{E}(\\cdot)$ denotes the expected value, and $\\mu_{i}$ denotes the mean of the feature vector $\\mathbf{x}_{i}$.\n",
    "The diagonal elements of the covariance matrix $\\Sigma_{ii}\\in\\mathbf{\\Sigma}$ are the variances of features $i$,\n",
    "while the off-diagonal elements $\\Sigma_{ij}\\in\\mathbf{\\Sigma}$ for $i\\neq{j}$ measure the relationship between features \n",
    "$i$ and $j$ in the dataset $\\mathcal{D}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "28e3681d-dd6f-4872-9967-16bc1666b1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32×32 Matrix{Float64}:\n",
       "  1.0          0.0339898   -0.0664774  …  -0.139084    -0.0650261\n",
       "  0.0339898    1.0         -0.032252      -0.00390982  -0.0337692\n",
       " -0.0664774   -0.032252     1.0            0.190691     0.417705\n",
       "  0.116338     0.00172211   0.100233      -0.101903     0.0452993\n",
       "  0.0442201   -0.0573173    0.105923       0.0361718    0.00331409\n",
       "  0.00591742   0.0315375   -0.0846749  …   0.01858      0.0799587\n",
       " -0.0947654   -0.0673657    0.242433       0.2639       0.293764\n",
       " -0.0882258    0.0791754    0.278398       0.0899397    0.208094\n",
       "  0.0688532   -0.0185064    0.319922       0.135729     0.191669\n",
       " -0.0411815   -0.148643     0.0139708     -0.0950178   -0.0852398\n",
       " -0.287338     0.144238     0.223241   …   0.110889     0.00641493\n",
       " -0.105303     0.105286     0.123968       0.110409    -0.19473\n",
       " -0.124966     0.20102     -0.0424669     -0.234802     0.0928804\n",
       "  ⋮                                    ⋱   ⋮           \n",
       " -0.0674445   -0.0900764    0.0574698  …   0.466954     0.00454851\n",
       " -0.0652994   -0.105649    -0.096555       0.0549397   -0.0946987\n",
       " -0.0944513    0.0374815    0.21946       -0.0997746   -0.0275777\n",
       "  0.180913     0.136217    -0.166882      -0.0358065    0.0119386\n",
       "  0.263648    -0.0847129    0.285381       0.117781     0.390958\n",
       " -0.217325    -0.0198559    0.150014   …   0.255073    -0.054344\n",
       " -0.0692304   -0.0144444    0.42356        0.111045     0.978322\n",
       "  0.0767461    0.105908    -0.0845413     -0.147594     0.477097\n",
       " -0.173883     0.00699948   0.202854       0.960412     0.252429\n",
       "  0.171716     0.0722143   -0.0431675     -0.0628259    0.0481605\n",
       " -0.139084    -0.00390982   0.190691   …   1.0          0.0986216\n",
       " -0.0650261   -0.0337692    0.417705       0.0986216    1.0"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Σ = cov(D) # transpose of D gives examples x examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17d732d-e631-418c-b342-04b61cc08c1c",
   "metadata": {},
   "source": [
    "Finally, we set some constants that we'll use throughout the lecture. Please look at the comment beside the constant value for its meaning, permissible values, units, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "79d0f502-456a-4cca-acd2-be6859cd808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_examples = size(D,1); # number of synthetic patients\n",
    "number_of_features = size(D,2); # number of features (measurements)\n",
    "maxiter = 1000; # maximum number of iterations\n",
    "ϵ = 1e-10; # stopping criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a344371-a7f4-4377-aada-441a1aa149d1",
   "metadata": {},
   "source": [
    "## Eigendecomposition\n",
    "Suppose we have a real square matrix $\\mathbf{A}\\in\\mathbb{R}^{m\\times{m}}$ which could be a measurement dataset, e.g., the columns of $\\mathbf{A}$ represent feature \n",
    "vectors $\\mathbf{x}_{1},\\dots,\\mathbf{x}_{m}$ or an adjacency array in a graph with $m$ nodes, etc. Eigenvalue-eigenvector problems involve finding a set of scalar values $\\left\\{\\lambda_{1},\\dots,\\lambda_{m}\\right\\}$ called \n",
    "[eigenvalues](https://mathworld.wolfram.com/Eigenvalue.html) and a set of linearly independent vectors \n",
    "$\\left\\{\\mathbf{v}_{1},\\dots,\\mathbf{v}_{m}\\right\\}$ called [eigenvectors](https://mathworld.wolfram.com/Eigenvector.html) such that:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{A}\\cdot\\mathbf{v}_{j} = \\lambda_{j}\\cdot\\mathbf{v}_{j}\\qquad{j=1,2,\\dots,m}\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\mathbf{v}\\in\\mathbb{C}^{m}$ and $\\lambda\\in\\mathbb{C}$. We can put the eigenvalues and eigenvectors together in matrix-vector form, which gives us an interesting matrix decomposition:\n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{V}\\cdot\\text{diag}(\\lambda)\\cdot\\mathbf{V}^{-1}\n",
    "$$\n",
    "where $\\mathbf{V}$ denotes the matrix of eigenvectors, where the eigenvectors form the columns of the matrix $\\mathbf{V}$, $\\text{diag}(\\lambda)$ denotes a diagonal matrix with the eigenvalues along the main diagonal, and $\\mathbf{V}^{-1}$ denotes the inverse of the eigenvalue matrix.\n",
    "\n",
    "__Symmetric real matricies__\n",
    "\n",
    "The eigendecomposition of a symmetric real matrix $\\mathbf{A}\\in\\mathbb{R}^{m\\times{m}}$ has some special properties. \n",
    "First, all the eigenvalues $\\left\\{\\lambda_{1},\\lambda_{2},\\dots,\\lambda_{m}\\right\\}$ of the matrix $\\mathbf{A}$ are real-valued.\n",
    "Next, the eigenvectors $\\left\\{\\mathbf{v}_{1},\\mathbf{v}_{2},\\dots,\\mathbf{v}_{m}\\right\\}$ of the matrix $\\mathbf{A}$ are orthogonal, i.e., $\\left<\\mathbf{v}_{i},\\mathbf{v}_{j}\\right> = 0$ for $i\\neq{j}$. Finally, the (normalized) eigenvectors $\\mathbf{v}_{j}/\\lVert|{\\mathbf{v}_{j}}\\rVert|$ of a symmetric real-valued matrix \n",
    "form an orthonormal basis for the space spanned by the matrix $\\mathbf{A}$ such that:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\left<\\hat{\\mathbf{v}}_{i},\\hat{\\mathbf{v}}_{j}\\right> = \\delta_{ij}\\qquad\\text{for}\\quad{i,j\\in\\mathbf{A}}\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\delta_{ij}$ is the Kronecker delta function. \n",
    "\n",
    "__So, why is this interesting__?\n",
    "* Eigenvectors represent fundamental directions of the matrix $\\mathbf{A}$. For the linear transformation defined by a matrix $\\mathbf{A}$, eigenvectors are the only vectors that do not change direction during the transformation. If we think about the matrix $\\mathbf{A}$ as a machine, we put the eigenvector $\\mathbf{v}_{\\star}$ into the machine, and we get back the same eigenvector $\\mathbf{v}_{\\star}$ multiplied by a scalar, the eigenvalue $\\lambda_{\\star}$.\n",
    "* Eigenvalues are scale factors for their eigenvector. An eigenvalue is a scalar that indicates how much a corresponding eigenvector is stretched or compressed during a linear transformation represented by the matrix $\\mathbf{A}$.\n",
    "* We can use the eigendecomposition to diagonalize the matrix $\\mathbf{A}$, i.e., transform the matrix into a diagonal form where the eigenvalues lie along the main diagonal. To see this, solve the eigendecomposition for the $\\text{diag}(\\lambda) = \\mathbf{V}^{-1}\\cdot\\mathbf{A}\\cdot\\mathbf{V}$. We can also use the eigenvalues to classify a matrix $\\mathbf{A}$ as positive (semi)definite or negative (semi)definite (which will be handy later).\n",
    "* If the matrix $\\mathbf{A}$ is symmetric, and all entries are positive, then all the eigenvalues will be real-valued, and the eigenvectors will be orthogonal (handy properties, as we shall soon see).\n",
    "\n",
    "Finally, another interpretation we'll also explore later is that eigenvectors represent the most critical directions in the data or system, and eigenvalues (or functions of them) represent their importance. Hmm, interesting. But how can we calculate them (given the buy versus build caveat above)?\n",
    "\n",
    "However, before we do anything, let's build the square matrix $\\mathbf{A}$ that we want to decompose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d156b2b4-d030-437a-97f0-bbbff404d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Σ; # let's use the covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f68694-55fd-4352-82e4-9956f7b48855",
   "metadata": {},
   "source": [
    "### Method 1: Power iteration\n",
    "The [power iteration method](https://en.wikipedia.org/wiki/Power_iteration) is an iterative algorithm to compute the largest eigenvalue and its corresponding eigenvector of a square (real) matrix; we'll consider only real-valued matrices here, but this approach can also be used for matrices with complex entries. \n",
    "\n",
    "* Perhaps the most famous application of [power iteration](https://en.wikipedia.org/wiki/Power_iteration) is the [Google PageRank algorithm](https://epubs.siam.org/doi/10.1137/050623280). Google's PageRank algorithm, which uses power iteration, utilizes the dominant eigenvalue and its corresponding eigenvector of a link connection matrix to assess the importance of web pages within a connection network.\n",
    "\n",
    "How does the [power iteration method](https://en.wikipedia.org/wiki/Power_iteration) work?\n",
    "\n",
    "__Phase 1: Eigenvector__: Suppose we have a real-valued square _diagonalizable_ matrix $\\mathbf{A}\\in\\mathbb{R}^{m\\times{m}}$ whose eigenvalues have the property $|\\lambda_{1}|\\geq|\\lambda_{2}|\\dots\\geq|\\lambda_{m}|$. Then, the eigenvector $\\mathbf{v}_{1}\\in\\mathbb{C}^{m}$ which corresponds to the largest eigenvalue $\\lambda_{1}\\in\\mathbb{C}$ can be (iteratively) estimated as:\n",
    "$$\n",
    "\\mathbf{v}_{1}^{(k+1)} = \\frac{\\mathbf{A}\\mathbf{v}_{1}^{(k)}}{\\Vert \\mathbf{A}\\mathbf{v}_{1}^{(k)} \\Vert}\\quad{k=0,1,2\\dots}\n",
    "$$\n",
    "\n",
    "where $\\lVert \\star \\rVert$ denotes the [L2 (Euclidean) vector norm](https://mathworld.wolfram.com/L2-Norm.html). The [power iteration method](https://en.wikipedia.org/wiki/Power_iteration) converges to a value for the eigenvector as $k\\rightarrow\\infty$ when a few properties are true, namely, $|\\lambda_{1}|/|\\lambda_{2}| < 1$ (which is unknown beforehand), and we pick an appropriate initial guess for $\\mathbf{v}_{1}$ (in our case, a random vector will work).\n",
    "\n",
    "__Phase 2: Eigenvalue__: Once we have an estimate for the eigenvector $\\hat{\\mathbf{v}}_{1}$, we can estimate the corresponding eigenvalue $\\hat{\\lambda}_{1}$ using [the Rayleigh quotient](https://en.wikipedia.org/wiki/Rayleigh_quotient). This argument proceeds from the definition of the eigenvalues and eigenvectors. We know, from the definition of eigenvalue-eigenvector pairs, that:\n",
    "$$\n",
    "\\mathbf{A}\\hat{\\mathbf{v}}_{1} - \\hat{\\lambda}_{1}\\hat{\\mathbf{v}}_{1}\\simeq{0}\n",
    "$$\n",
    "where we use the $\\simeq$ symbol because we don't have the true eigenvector $\\mathbf{v}_{1}$, only an estimate of it. To solve this expression for the (estimated) eigenvalue $\\hat{\\lambda}_{1}$, we multiply through by the transpose of the eigenvector and solve for the eigenvalue:\n",
    "$$\n",
    "\\hat{\\lambda}_{1} \\simeq \\frac{\\hat{\\mathbf{v}}_{1}^{T}\\mathbf{A}\\hat{\\mathbf{v}}_{1}}{\\hat{\\mathbf{v}}_{1}^{T}\\hat{\\mathbf{v}}_{1}} = \\frac{\\left<\\mathbf{A}\\hat{\\mathbf{v}}_{1},\\hat{\\mathbf{v}}_{1}\\right>}{\\left<\\hat{\\mathbf{v}}_{1},\\hat{\\mathbf{v}}_{1}\\right>}\n",
    "$$\n",
    "where $\\left<\\star,\\star\\right>$ denotes [an inner product](https://mathworld.wolfram.com/InnerProduct.html). \n",
    "\n",
    "__Algorithm__\n",
    "* __Initialization__. We begin (iteration $k=0$) with an initial (random) guess of the eigenvector $\\mathbf{v}_{1}^{(0)}$, the maximum number of iterations we are willing to take `maxiter,` and a tolerance parameter $\\epsilon>0$.  \n",
    "* __Update__: Next, we repeatedly multiply the $\\mathbf{v}^{\\star}_{1}$ vector by the matrix $\\mathbf{A}$ and normalize the result by $\\Vert\\mathbf{A}\\mathbf{v}^{\\star}_{1}\\Vert$. This iterative approach capitalizes on the property that the dominant eigenvalue will exert the most influence on the vector $\\mathbf{v}$ over successive iterations, allowing it to converge towards the eigenvector associated with the largest eigenvalue.\n",
    "* __Stopping__: We stop the iteration procedure after `maxiter` number of iterations is reached or when the difference between successive iterations is _small_ in some sense, i.e., $\\lVert \\mathbf{v}_{1}^{(k)} - \\mathbf{v}_{1}^{(k-1)} \\rVert\\leq\\epsilon$ where $\\lVert\\star\\rVert$ is [some vector norm](https://mathworld.wolfram.com/VectorNorm.html). In practice, we'll use both stopping criteria and the L2 norm to guard against an infinite loop (our iteration will be implemented using [a `while` loop](https://docs.julialang.org/en/v1/base/base/#while)).\n",
    "\n",
    "While simple to implement, the [power iteration method](https://en.wikipedia.org/wiki/Power_iteration) may exhibit slow convergence, mainly when the largest eigenvalue is close in magnitude to other eigenvalues, i.e., $|\\lambda_{1}|/|\\lambda_{2}| \\sim 1$.\n",
    "Check out a [power iteration pseudo-code in the course notes](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-2/L2a/docs/Notes.pdf)\n",
    "\n",
    "__Additional references__:\n",
    "* [Prof. David Bindel: Cornell CS 6210: Matrix Computations (2016), Lecture on Power Iteration](https://www.cs.cornell.edu/~bindel/class/cs6210-f16/lec/2016-10-17.pdf)\n",
    "* [Prof. Tom Trogdon: UCI MATH 105A: Numerical Analysis (2016), Lecture 22: The Power Method](https://faculty.washington.edu/trogdon/105A/html/Lecture22.html)\n",
    "* [Prof. Thomas Strohmer: UCD MATH 108: Mathematical Algorithms for Artificial Intelligence and Big Data (2017), Lecture on PageRank](https://math.ucdavis.edu/%7Estrohmer/courses/180BigData/180lecture_pagerank.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b33aa3-9663-4583-92ed-a51f2684ea49",
   "metadata": {},
   "source": [
    "We've implemented the [`poweriteration(...)` method](https://en.wikipedia.org/wiki/Power_iteration) in the [`poweriteration(...)` function](src/Compute.jl). \n",
    "* The [`poweriteration(...)` function](src/Compute.jl) takes the square matrix $\\mathbf{A}$, an initial guess for the eigenvector $\\mathbf{v}^{(0)}_{1}$ and (optional) keyword parameters controlling the stopping criteria as arguments. The function returns a tuple holding the estimated eigenvector $\\hat{\\mathbf{v}}_{1}$ and eigenvalue $\\hat{\\lambda}_{1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9e86e281-4205-4071-bf8e-16145097eb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 134 iterations\n"
     ]
    }
   ],
   "source": [
    "(v̂,λ̂) = let\n",
    "\n",
    "    n = size(A,1); # how many rows (cols) do we have? (square)\n",
    "    vₒ = randn(n); # initial random guess\n",
    "\n",
    "    # call the poweriteration function\n",
    "    (v, λ) = poweriteration(A, vₒ, maxiter = maxiter, ϵ = ϵ);\n",
    "\n",
    "    # return -\n",
    "    (v,λ)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e77bf0-02a0-484d-863f-96aeedc3045d",
   "metadata": {},
   "source": [
    "To test our power iteration implementation, let's compare the values of the largest eigenvalue, eigenvector pair $(\\hat{\\lambda}_{1}, \\hat{\\mathbf{v}}_{1})$ that we just estimated with those computed [using the `eigen(...)` function exported by the LinearAlgebra.jl package](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigen). The built-in [`eigen(...)` function](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigen) takes the matrix $\\mathbf{A}$ (and some additional optional arguments) and returns [an `Eigen` factorization object](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.Eigen) holding the eigenvalues and eigenvectors.\n",
    "* __Check__: We use the [`@assert` macro](https://docs.julialang.org/en/v1/base/base/#Base.@assert) in combination with the shortcut version of [the `isapprox(...)` function](https://docs.julialang.org/en/v1/base/math/#Base.isapprox) to compare our result to the built in function. If the argument to [the `@assert` macro](https://docs.julialang.org/en/v1/base/base/#Base.@assert) evaluates to `false`, an [`AssertionError` instance](https://docs.julialang.org/en/v1/base/base/#Core.AssertionError) is thrown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c91e29b8-b4ea-416b-9d7c-612adc22b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    F = eigen(A); # compute the eigendecomposition\n",
    "\n",
    "    # get\n",
    "    λ = maximum(F.values); # get the max eigenvalue (sorted, this should be the last element)\n",
    "    v = F.vectors[:,end]; # eigenvectors are sorted - the last column is v₁\n",
    "\n",
    "    # tests\n",
    "    @assert (λ ≈ λ̂) && (abs.(v) ≈ abs.(v̂))  # do the eigenvalues and eigenvectors match?\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4080a7-b124-4045-801d-121835f0042f",
   "metadata": {},
   "source": [
    "### Method 2: QR Iteration\n",
    "[QR iteration](https://en.wikipedia.org/wiki/QR_algorithm) is a fundamental technique in numerical linear algebra, primarily used for computing the eigenvalues and eigenvectors of matrices. The algorithm leverages the concept of [QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition), which expresses a (rectangular) matrix $\\mathbf{A}\\in\\mathbb{R}^{n\\times{m}}$ as a product of an orthogonal matrix $\\mathbf{Q}\\in\\mathbb{R}^{n\\times{n}}$ and an upper triangular matrix $\\mathbf{R}\\in\\mathbb{R}^{n\\times{m}}$:\n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{Q}\\mathbf{R}\n",
    "$$\n",
    "where $\\mathbf{Q}^{T}\\mathbf{Q} = \\mathbf{I}$. The core of the QR iteration algorithm involves iteratively decomposing a given matrix $\\mathbf{A}$ into its $\\mathbf{Q}$ and $\\mathbf{R}$ factors and then reformulating the matrix for subsequent iterations. Under certain conditions, [QR iteration](https://en.wikipedia.org/wiki/QR_algorithm) will converge to a triangular matrix with the eigenvalues of the original matrix $\\mathbf{A}$ listed on the diagonal.\n",
    "\n",
    "__Algorithm__\n",
    "* __Initialization__. We begin by specifying an initial matrix $\\mathbf{A}_{1} = \\mathbf{A}$, the maximum number of iterations `maxiter` that we are willing to do, and a tolerance parameter $\\epsilon$.\n",
    "* __Update__. For iteration $k = 1,2,\\dots$, compute the [QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition) of $\\mathbf{A}_{k} = \\mathbf{Q}_{k}\\mathbf{R}_{k}$. We then form a new matrix $\\mathbf{A}_{k+1} = \\mathbf{R}_{k}\\mathbf{Q}_{k}$, which can be re-written as $\\mathbf{A}_{k+1} = \\mathbf{Q}^{T}_{k}\\mathbf{A}_{k}\\mathbf{Q}_{k}$.\n",
    "* __Stopping__. We stop the iteration procedure after `maxiter` iterations is reached or when the difference between successive iterations is _small_ in some sense, i.e., $\\lVert \\mathbf{A}_{k+1} - \\mathbf{A}_{k} \\rVert_{1}\\leq\\epsilon$ where $\\lVert\\star\\rVert_{1}$ denotes the [p = 1 matrix norm](https://en.wikipedia.org/wiki/Matrix_norm), or perhaps $\\lVert \\lambda_{k+1} - \\mathbf{\\lambda}_{k} \\rVert_{2}\\leq\\epsilon$ where $\\lVert\\star\\rVert_{2}$ is [the L2-vector norm](https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm), i.e., the eigenvalues don't change between iterations, and $\\epsilon$ is a tolerance parameter.\n",
    "\n",
    "Once we have converged to matrix $\\mathbf{A}_{\\star}$, we get the eigenvalue from the diagonal of $\\mathbf{A}_{\\star}$. To compute the eigenvectors, we solve the homogenous system of linear algebraic equations:\n",
    "$$\n",
    "\\left(\\mathbf{A} - \\lambda_{\\star}\\mathbf{I} \\right)\\cdot\\mathbf{v}_{\\star} = \\mathbf{0}\n",
    "$$\n",
    "\n",
    "Before we implement [QR iteration](https://en.wikipedia.org/wiki/QR_algorithm), let's look at how to compute the $\\mathbf{Q}$ and $\\mathbf{R}$ matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539e979a-b72c-430d-9b31-10e197f646d8",
   "metadata": {},
   "source": [
    "#### Aside: Classical and Modified Gram-Schmidt\n",
    "The [QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition) can be computed using a variety of approaches, including a handy technique called [the Gram–Schmidt algorithm](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process). Gram-Schmidt computes a set of two or more vectors that are orthogonal (perpendicular) to each other. The vector projection operation is the basic unit operation of [the Gram–Schmidt algorithm](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process). The projection of vector $\\mathbf{v}$ on a nonzero vector $\\mathbf{u}$ is given by: \n",
    "$$\n",
    "P_{\\mathbf{u}}(\\mathbf{v}) = \\frac{\\left<\\mathbf{v},\\mathbf{u}\\right>}{\\left<\\mathbf{u},\\mathbf{u}\\right>}\\cdot\\mathbf{u}\n",
    "$$\n",
    "where $\\left<\\star,\\star\\right>$ denotes [an inner product](https://mathworld.wolfram.com/InnerProduct.html).  Classical Gram-Schmidt allows us to compute a set of orthogonal vectors using the projection operation shown above, i.e., given vectors $\\mathbf{v}_{1},\\mathbf{v}_{2},\\dots,\\mathbf{v}_{k}$ the classical Gram–Schmidt process defines the vectors $\\mathbf{u}_{1},\\mathbf{u}_{2},\\dots,\\mathbf{u}_{k}$ as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{u}_{1} & = \\mathbf{v}_{1} \\\\\n",
    "\\mathbf{u}_{2} & = \\mathbf{v}_{2} - P_{\\mathbf{u}_{1}}(\\mathbf{v}_{2})\\\\\n",
    "\\mathbf{u}_{3} & = \\mathbf{v}_{3} - P_{\\mathbf{u}_{1}}(\\mathbf{v}_{2}) - P_{\\mathbf{u}_{2}}(\\mathbf{v}_{3})\\\\\n",
    "\\vdots & =  \\vdots \\\\\n",
    "\\mathbf{u}_{k} & = \\mathbf{v}_{k} - \\sum_{j=1}^{k-1}P_{\\mathbf{u}_{j}}(\\mathbf{v}_{k})\n",
    "\\end{align*}\n",
    "$$\n",
    "Classical Gram-Schmidt can sometimes produce _almost_ orthogonal vectors because of roundoff error. Let's look at a classic Gram-Schmidt example before we fix this issue and compute our $\\mathbf{Q}$ and $\\mathbf{R}$. Check out the [Classical and Modified Gram-Schmidt pseudo-code in the course notes](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-2/L2a/docs/Notes.pdf)\n",
    "\n",
    "__Additional references__:\n",
    "* [Prof. Tom Trogdon: UCI MATH 105A: Numerical Analysis (2016), Lecture 21: Orthogonal Matricies](https://faculty.washington.edu/trogdon/105A/html/Lecture21.html)\n",
    "* [Prof. Tom Trogdon: UCI MATH 105A: Numerical Analysis (2016), Lecture 23: The modified Gram-Schmidt procedure](https://faculty.washington.edu/trogdon/105A/html/Lecture23.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9a16764c-4063-4a89-ac9b-8e549ce594e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(T, QCGS, QMGS) = let\n",
    "    T = randn(10,3);\n",
    "    QCGS = orthogonalize(T, ClassicalGramSchmidtAlgorithm())\n",
    "    QMGS = orthogonalize(T, ClassicalGramSchmidtAlgorithm())\n",
    "    T,QCGS,QMGS # return\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06f7ff1-061b-49ea-bcd6-f59e29f14c2e",
   "metadata": {},
   "source": [
    "Once we have $\\mathbf{Q}$, we can compute the $\\mathbf{R}$ factor as:\n",
    "$$\n",
    "\\mathbf{R} = \\mathbf{Q}^{\\top}\\mathbf{A}\n",
    "$$\n",
    "which takes advantage of the properties of orthogonal matrices, namely, $\\mathbf{Q}^{\\top}\\mathbf{Q} = \\mathbf{I}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5ee1ed29-f25a-455e-bdbf-fbeeeea32e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = transpose(QCGS)*T;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd2e969-6011-42e8-9e47-90d77356018f",
   "metadata": {},
   "source": [
    "Are the columns of $\\mathbf{Q}$ orthonomal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "834e7997-ec18-482a-8982-83448a7973c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.7515620786151527e-17"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot(QCGS[:,2],QCGS[:,3]) # inner product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "44865b2f-1be6-4ccf-a2b5-ad92ca999e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Float64}:\n",
       "  1.0          -2.23377e-16   1.26873e-16\n",
       " -2.23377e-16   1.0          -2.75156e-17\n",
       "  1.26873e-16  -2.75156e-17   1.0"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transpose(QCGS)*QCGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce742392-4c56-4ad7-817f-7d8adfebafd3",
   "metadata": {},
   "source": [
    "#### Compute eigenvalues and eigenvectors using QR iteration\n",
    "We can use [QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition) to compute the eigenvectors and eigenvalues of a square matrix $\\mathbf{A}$. We've implemented our QR own iteration algorithm in [the `myqriteration(...)` method](src/Eigendecomposition.jl). This code uses [the `qr(...)` function exported by LinearAlgebra.jl](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.qr) to compute the [QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition) (instead of [our `orthogonalize(...)` function](src/Compute.jl) discussed above).\n",
    "* The [`myqriteration(...)` method](src/Eigendecomposition.jl) takes the matrix $\\mathbf{A}$ that we want to decompose, along with the optional `maxiter` and $\\epsilon$ tolerance parameters. This method returns the sorted eigenvalues and their corresponding eigenvalues organized in [a `Tuple`](https://docs.julialang.org/en/v1/base/base/#Core.Tuple). We'll process this data and return $\\text{diag}(\\lambda)$ and $\\mathbf{V}$, i.e., the diagonal matrix of eigenvalues and the matrix of eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e14123dc-5ccc-4371-8e5c-1379c3a8bdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 727 iterations\n"
     ]
    }
   ],
   "source": [
    "myeiegnresult = myqriteration(A, maxiter = maxiter, ϵ = ϵ);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "59749622-3c0e-41d3-b2fc-cf66e618d4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 727 iterations\n"
     ]
    }
   ],
   "source": [
    "(Λ̂,V̂) = let\n",
    "\n",
    "    # initialize -\n",
    "    (n,m) = size(A); # what is the dimension of A?\n",
    "    Λ = Matrix{Float64}(1.0*I, n, n); # builds the I matrix, we'll update with λ -\n",
    "    V = Array{Float64,2}(undef, n,n); # builds an empty V matrix\n",
    "\n",
    "    # call our qr-iteration method\n",
    "    myeiegnresult = myqriteration(A, maxiter = maxiter, ϵ = ϵ);\n",
    "\n",
    "    # package the eigenvalues into Λ -\n",
    "    for i ∈ 1:n\n",
    "        Λ[i,i] = myeiegnresult[1][i];\n",
    "    end\n",
    "\n",
    "    # package the eigenvectors into the V-matrix\n",
    "    for i ∈ 1:n\n",
    "        v = myeiegnresult[2][i]; # this gets the ith eigenvector\n",
    "        for j ∈ 1:n\n",
    "            V[j,i] = v[j];\n",
    "        end\n",
    "    end\n",
    "\n",
    "    Λ,V\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9036bf0-1cbf-4de4-a2cb-eb9050416469",
   "metadata": {},
   "source": [
    "##### Check: Eigenvalues\n",
    "Let's check if the eigenvalues computed by the [`eigen(...)` function](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigen) are the same as the ones we just calculated [by our `myqriteration(...)` implementation](src/Compute.jl). First, let's use the [built-in eigen function](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigen) and see what we get. The [eigen function](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigen) takes a square array `A` as an argument and returns the eigendecomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2525546b-f1c1-4e20-b3d0-cc7fa360e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Λ,V) = let\n",
    "\n",
    "    # initialize -\n",
    "    (n,m) = size(A); # what is the dimension of A?\n",
    "    Λ = Matrix{Float64}(1.0*I, n, n); # builds the I matrix, we'll update with λ -\n",
    "    \n",
    "    # Decompose using the built-in function\n",
    "    F = eigen(A);   # eigenvalues and vectors in F of type Eigen\n",
    "    λ = F.values;   # vector of eigenvalues\n",
    "    V = F.vectors;  # n x n matrix of eigenvectors, each col is an eigenvector\n",
    "\n",
    "    # package the eigenvalues into Λ -\n",
    "    for i ∈ 1:n\n",
    "        Λ[i,i] = λ[i];\n",
    "    end\n",
    "\n",
    "    Λ,V\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e5cc9b-aea1-4106-b89f-1be0bdc5e874",
   "metadata": {},
   "source": [
    "How far apart are the eigenvalues estimated using the builtin function versus our qr-iteration implementation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5d0472c7-baad-434c-bef8-3bae9acbd873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7292091353942218e-9"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(diag(Λ) - diag(Λ̂)) # is this is small (?), we are good to go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad79ac55-ede7-4a61-8b7d-2dcdaa50a960",
   "metadata": {},
   "source": [
    "##### Check: Eigenvectors\n",
    "Let's do the same thing with the eigenvectors. How similar are our eigenvectors computed using [the `myqriteration(...)` function](src/Compute.jl) to those calculated using [the `eigen(...)` method](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigen)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "06e1c1b1-1ba7-42ab-9de7-19757f008ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9437218334167614e-13"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 30; # which eigenvector do I want to check?\n",
    "norm(abs.(V[:,i]) - abs.(V̂[:,i])) # my |v| are ok, but why |*|?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf940c5d-a76d-4367-af19-3569782b8c15",
   "metadata": {},
   "source": [
    "# Today?\n",
    "What did we do today? Give me three things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4c753-51ed-42ea-983f-ed350c19406b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
