{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d0c28b-498e-4e20-a8ea-8ea5a5aecbac",
   "metadata": {},
   "source": [
    "# Lecture 2a: Eigendecomposition of Data and Systems\n",
    "In this lecture, we will discuss the eigendecomposition of a square matrix and how it can be used to understand data and systems in unsupervised machine learning. There are several key ideas in this lecture:\n",
    "\n",
    "* __Eigendecomposition__ allows us to decompose a matrix into its constituent parts, the [eigenvectors and eigenvalues](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors). These values help us understand the structure of the data or system represented by the matrix. We'll look at two approaches to estimate the [eigenvalues and eigenvectors](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors) of a matrix:\n",
    "    * __Power iteration method__ estimates the _largest_ eigenvalue/eigenvector pair. Given a _diagonalizable_ matrix $\\mathbf{A}$ the power iteration algorithm will produce a number $\\lambda$, which is the greatest (in absolute value) eigenvalue of $\\mathbf{A}$ and a nonzero vector $\\mathbf{v}$ which is a corresponding eigenvector of $\\lambda$ such that $\\mathbf{A}\\mathbf{v} = \\lambda\\cdot\\mathbf{v}$.\n",
    "    * __QR iteration__ is another approach to compute the eigendecomposition of the matrix $\\mathbf{A}$. However, unlike power iteration, this approach will give all eigenvalues and eigenvectors of the matrix $\\mathbf{A}$. The QR factorization algorithm relies on the [QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition), which itself relies on the [Gram-Schmidt algorithm](https://en.wikipedia.org/wiki/Gram–Schmidt_process).\n",
    "* __Buy versus build__: While we will explore these two approaches to compute the eigendecomposition of a matrix in the lecture and associated lab, most, if not all, computing platforms already have built-in functionality to do this computation. For example, [Julia has the `eigen(...)` function exported by the LinearAlgebra.jl package](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigen). Most of the time, there is no need to recreate the wheel (and your implementation will likely be worse in space and time complexity). So buy!\n",
    "\n",
    "Lecture notes can be found: [here!](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-2/L2a/docs/Notes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9085c0c-4cbd-4e22-9b47-1127307d413f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setup, Data and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. The `Include.jl` file loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc0dc6e0-1af4-4406-af0f-67e920dac536",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde34514-8a96-4197-80e3-8aaa98834ab7",
   "metadata": {},
   "source": [
    "We'll use the coagulation dataset. Let's load this data from disk using [the `MySyntheticDataSet()` function](src/Files.jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afab24e4-49f5-4235-bad3-014cfe722e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MySyntheticDataset() |> d-> d[\"ensemble\"]; "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2e02d2-4996-4279-acd9-ff1c2c99c622",
   "metadata": {},
   "source": [
    "The keys of the dataset dictionary are the `actual` patient indexes. These keys point to `synthetic` patient measurement vectors constructed by building a model of the original data distribution. To explore this data, specify an original patient index (one of the keys of the original dictionary) in the `original_patient_index::Int` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b5d4612-a983-41c4-965a-d47fbe0ed083",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_patient_index = 7; # i ∈ {keys}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ef3440-055c-4a50-b5bd-d4f9eab9f87a",
   "metadata": {},
   "source": [
    "Next, we'll build a data matrix with the `synthetic` measurement vectors for the specified original patient index. We'll store this in the `D::Array{<:Number, 1}` matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eb22fb3-5846-48a1-b8fe-2330a8d921e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = let\n",
    "\n",
    "    M = dataset[original_patient_index];\n",
    "    number_of_rows = length(M); # number of synthetic patients\n",
    "    number_of_cols = length(M[1]) - 1; # number of measurements (features), first col is the visit number\n",
    "    D = Array{Float64,2}(undef, number_of_rows, number_of_cols);\n",
    "\n",
    "    for i ∈ 0:(number_of_rows - 1)\n",
    "        for j ∈ 1:(number_of_cols)\n",
    "            D[i+1,j] = M[i][j+1];\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # D̂ = copy(D); # no z-scale this data\n",
    "    # for j ∈ 1:number_of_cols\n",
    "    #     sample_vector = D[:,j]; \n",
    "    #     μ = mean(sample_vector);\n",
    "    #     σ = std(sample_vector);\n",
    "\n",
    "    #     for i ∈ 1:number_of_rows\n",
    "    #         D̂[i,j] = (sample_vector[i] - μ)/σ;\n",
    "    #     end\n",
    "    # end\n",
    "    \n",
    "    D\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17d732d-e631-418c-b342-04b61cc08c1c",
   "metadata": {},
   "source": [
    "Finally, we set some constants that we'll use throughout the lecture. See the comment beside the constant value for its meaning, permissible values, units, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79d0f502-456a-4cca-acd2-be6859cd808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_examples = size(D,1); # number of synthetic patients\n",
    "number_of_features = size(D,2); # number of features (measurements)\n",
    "maxiter = 100; # maximum number of iterations\n",
    "ϵ = 1e-8; # stopping criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a344371-a7f4-4377-aada-441a1aa149d1",
   "metadata": {},
   "source": [
    "## Eigendecomposition\n",
    "Suppose we have a real square matrix $\\mathbf{A}\\in\\mathbb{R}^{m\\times{m}}$ which could be a measurement dataset, e.g., the columns of $\\mathbf{A}$ represent feature \n",
    "vectors $\\mathbf{x}_{1},\\dots,\\mathbf{x}_{m}$ or an incidence array in a graph with $m$ nodes, etc. Eigenvalue-eigenvector problems involve finding a set of scalar values $\\left\\{\\lambda_{1},\\dots,\\lambda_{m}\\right\\}$ called \n",
    "[eigenvalues](https://mathworld.wolfram.com/Eigenvalue.html) and a set of linearly independent vectors \n",
    "$\\left\\{\\mathbf{v}_{1},\\dots,\\mathbf{v}_{m}\\right\\}$ called [eigenvectors](https://mathworld.wolfram.com/Eigenvector.html) such that:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{A}\\cdot\\mathbf{v}_{j} = \\lambda_{j}\\cdot\\mathbf{v}_{j}\\qquad{j=1,2,\\dots,m}\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\mathbf{v}\\in\\mathbb{C}^{m}$ and $\\lambda\\in\\mathbb{C}$. So, why is this interesting?\n",
    "* Eigenvectors represent fundamental directions of the matrix $\\mathbf{A}$. For the linear transformation defined by a matrix $\\mathbf{A}$, eigenvectors are the only vectors that do not change direction during the transformation. If we think about the matrix $\\mathbf{A}$ as a machine, we put the eigenvector $\\mathbf{v}_{\\star}$ into the machine, and we get back the same eigenvector $\\mathbf{v}_{\\star}$ multiplied by a scalar, the eigenvalue $\\lambda_{\\star}$.\n",
    "* Eigenvalues are scale factors for their eigenvector. An eigenvalue is a scalar that indicates how much a corresponding eigenvector is stretched or compressed during a linear transformation represented by the matrix $\\mathbf{A}$.\n",
    "\n",
    "Another interpretation we'll explore later is that eigenvectors represent the most critical directions in the data or system, and the eigenvalues represent the importance of these directions. Hmmm, interesting. But how can we calculate them (given the buy versus build caveat above)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f68694-55fd-4352-82e4-9956f7b48855",
   "metadata": {},
   "source": [
    "### Method 1: Power iteration\n",
    "The [power iteration method](https://en.wikipedia.org/wiki/Power_iteration) is an iterative algorithm to compute the largest eigenvalue and its corresponding eigenvector of a square (real) matrix; we'll consider only real-valued matrices here, but this approach can also be used for matrices with complex entries. \n",
    "\n",
    "__Phase 1: Eigenvector__: Suppose we have a real-valued square _diagonalizable_ matrix $\\mathbf{A}\\in\\mathbb{R}^{m\\times{m}}$ whose eigenvalues have the property $|\\lambda_{1}|\\geq|\\lambda_{2}|\\dots\\geq|\\lambda_{m}|$. Then, the eigenvector $\\mathbf{v}_{1}\\in\\mathbb{C}^{m}$ which corresponds to the largest eigenvalue $\\lambda_{1}\\in\\mathbb{C}$ can be (iteratively) estimated as:\n",
    "$$\n",
    "\\mathbf{v}_{1}^{(k+1)} = \\frac{\\mathbf{A}\\mathbf{v}_{1}^{(k)}}{\\Vert \\mathbf{A}\\mathbf{v}_{1}^{(k)} \\Vert}\\quad{k=0,1,2\\dots}\n",
    "$$\n",
    "\n",
    "where $\\lVert \\star \\rVert$ denotes the [L2 (Euclidean) vector norm](https://mathworld.wolfram.com/L2-Norm.html). The [power iteration method](https://en.wikipedia.org/wiki/Power_iteration) converges to a value for the eigenvector as $k\\rightarrow\\infty$ when a few properties are true, namely, $|\\lambda_{1}|/|\\lambda_{2}| < 1$ (which is unknown beforehand), and we pick an appropriate initial guess for $\\mathbf{v}_{1}$ (in our case, a random vector will work).\n",
    "\n",
    "__Phase 2: Eigenvalue__: Once we have an estimate for the eigenvector $\\hat{\\mathbf{v}}_{1}$, we can estimate the corresponding eigenvalue $\\hat{\\lambda}_{1}$ using [the Rayleigh quotient](https://en.wikipedia.org/wiki/Rayleigh_quotient). This argument proceeds from the definition of the eigenvalues and eigenvectors. We know, from the definition of eigenvalue-eigenvector pairs, that:\n",
    "$$\n",
    "\\mathbf{A}\\hat{\\mathbf{v}}_{1} - \\hat{\\lambda}_{1}\\hat{\\mathbf{v}}_{1}\\simeq{0}\n",
    "$$\n",
    "where we use the $\\simeq$ symbol because we don't have the true eigenvector $\\mathbf{v}_{1}$, only an estimate of it. To solve this expression for the (estimated) eigenvalue $\\hat{\\lambda}_{1}$, we multiply through by the transpose of the eigenvector and solve for the eigenvalue:\n",
    "$$\n",
    "\\hat{\\lambda}_{1} \\simeq \\frac{\\hat{\\mathbf{v}}_{1}^{T}\\mathbf{A}\\hat{\\mathbf{v}}_{1}}{\\hat{\\mathbf{v}}_{1}^{T}\\hat{\\mathbf{v}}_{1}} = \\frac{\\left<\\mathbf{A}\\hat{\\mathbf{v}}_{1},\\hat{\\mathbf{v}}_{1}\\right>}{\\left<\\hat{\\mathbf{v}}_{1},\\hat{\\mathbf{v}}_{1}\\right>}\n",
    "$$\n",
    "where $\\left<\\star,\\star\\right>$ denotes [an inner product](https://mathworld.wolfram.com/InnerProduct.html). \n",
    "\n",
    "__Algorithm__\n",
    "* __Initialization__. We begin (iteration $k=0$) with an initial (random) guess of the eigenvector $\\mathbf{v}_{1}^{(0)}$, the maximum number of iterations we are willing to take `maxiter,` and a tolerance parameter $\\epsilon>0$.  \n",
    "* __Update__: Next, we repeatedly multiply the $\\mathbf{v}^{\\star}_{1}$ vector by the matrix $\\mathbf{A}$ and normalize the result by $\\Vert\\mathbf{A}\\mathbf{v}^{\\star}_{1}\\Vert$. This iterative approach capitalizes on the property that the dominant eigenvalue will exert the most influence on the vector $\\mathbf{v}$ over successive iterations, allowing it to converge towards the eigenvector associated with the largest eigenvalue.\n",
    "* __Stopping__: We stop the iteration procedure after `maxiter` number of iterations is reached or when the difference between successive iterations is _small_ in some sense, i.e., $\\lVert \\mathbf{v}_{1}^{(k)} - \\mathbf{v}_{1}^{(k-1)} \\rVert\\leq\\epsilon$ where $\\lVert\\star\\rVert$ is [some vector norm](https://mathworld.wolfram.com/VectorNorm.html). In practice, we'll use both stopping criteria and the L2 norm to guard against an infinite loop (our iteration will be implemented using [a `while` loop](https://docs.julialang.org/en/v1/base/base/#while)).\n",
    "\n",
    "While simple to implement, the [power iteration method](https://en.wikipedia.org/wiki/Power_iteration) may exhibit slow convergence, mainly when the largest eigenvalue is close in magnitude to other eigenvalues, i.e., $|\\lambda_{1}|/|\\lambda_{2}| \\sim 1$.\n",
    "Check out a [power iteration pseudo-code here!](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-1/L1c/figs/pcode-kmeans.pdf)\n",
    "\n",
    "Additional references:\n",
    "* https://www.cs.cornell.edu/~bindel/class/cs6210-f16/lec/2016-10-17.pdf\n",
    "* https://blogs.sas.com/content/iml/2012/05/09/the-power-method.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b33aa3-9663-4583-92ed-a51f2684ea49",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "We've implemented the [power iteration method](https://en.wikipedia.org/wiki/Power_iteration) in the [`poweriteration(...)` function](src/Compute.jl). \n",
    "* The [`power iteration (...)` function](src/Compute.jl) takes the square matrix $\\mathbf{A}$, an initial guess for the eigenvector $\\mathbf{v}^{(0)}_{1}$ and (optional) keyword parameters controlling the stopping criteria as arguments. The function returns a tuple holding the estimated eigenvector $\\hat{\\mathbf{v}}_{1}$ and eigenvalue $\\hat{\\lambda}_{1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e86e281-4205-4071-bf8e-16145097eb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 4 iterations\n"
     ]
    }
   ],
   "source": [
    "(v̂,λ̂) = let\n",
    "\n",
    "    A = transpose(D)*D; # build a square matrix from the data D^T*D gives measure x measure, D*D^T gives patient x patient \n",
    "    n = size(A,1); # how many rows (cols) do we have? (square)\n",
    "    vₒ = randn(n); # initial random guess\n",
    "\n",
    "    # call the poweriteration function\n",
    "    (v, λ) = poweriteration(A, vₒ, maxiter = maxiter, ϵ = ϵ);\n",
    "\n",
    "    # return -\n",
    "    (v,λ)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e77bf0-02a0-484d-863f-96aeedc3045d",
   "metadata": {},
   "source": [
    "#### Test\n",
    "To test our implementation, let's compare the values of $(\\hat{\\lambda}_{1}, \\hat{\\mathbf{v}}_{1})$ that we estimated with those computed [using the `eigen(...)` function exported by the LinearAlgebra.jl package](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigen). \n",
    "* The built-in [`eigen(...)` function](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigen) takes the matrix $\\mathbf{A}$ (and some additional optional arguments) and returns [an `Eigen` factorization object](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.Eigen) holding the eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c91e29b8-b4ea-416b-9d7c-612adc22b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    A = transpose(D)*D; # build a square matrix from the data\n",
    "    F = eigen(A); # compute the eigendecomposition\n",
    "\n",
    "    # get\n",
    "    λ = maximum(F.values); # get the max eigenvalue (sorted, this should be the last element)\n",
    "    v = F.vectors[:,end]; # eigenvectors are sorted - the last column is v₁\n",
    "\n",
    "    # tests\n",
    "    @assert (λ ≈ λ̂) && (abs.(v) ≈ abs.(v̂))  # do the eigenvalues and eigenvectors match?\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4080a7-b124-4045-801d-121835f0042f",
   "metadata": {},
   "source": [
    "### Method 2: QR Iteration\n",
    "[QR iteration](https://en.wikipedia.org/wiki/QR_algorithm) is a fundamental technique in numerical linear algebra, primarily used for computing the eigenvalues and eigenvectors of matrices. The algorithm leverages the concept of [QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition), which expresses a (rectangular) matrix $\\mathbf{A}\\in\\mathbb{R}^{n\\times{m}}$ as a product of an orthogonal matrix $\\mathbf{Q}\\in\\mathbb{R}^{n\\times{n}}$ and an upper triangular matrix $\\mathbf{R}\\in\\mathbb{R}^{n\\times{m}}$:\n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{Q}\\mathbf{R}\n",
    "$$\n",
    "where $\\mathbf{Q}^{T}\\mathbf{Q} = \\mathbf{I}$. The core of the QR iteration algorithm involves iteratively decomposing a given matrix $\\mathbf{A}$ into its $\\mathbf{Q}$ and $\\mathbf{R}$ factors and then reformulating the matrix for subsequent iterations. \n",
    "\n",
    "__Algorithm__\n",
    "* __Initialization__. We begin by specifying an initial matrix $\\mathbf{A}_{1} = \\mathbf{A}$, the maximum number of iterations `maxiter` that we are willing to do, and a tolerance parameter $\\epsilon$.\n",
    "* __Update__. For iteration $k = 1,2,\\dots$, compute the [QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition) of $\\mathbf{A}_{k} = \\mathbf{Q}_{k}\\mathbf{R}_{k}$. We then form a new matrix $\\mathbf{A}_{k+1} = \\mathbf{R}_{k}\\mathbf{Q}_{k}$, which can be re-written as $\\mathbf{A}_{k+1} = \\mathbf{Q}^{T}_{k}\\mathbf{A}_{k}\\mathbf{Q}_{k}$.\n",
    "* __Stopping__. We stop the iteration procedure after `maxiter` number of iterations is reached or when the difference between successive iterations is _small_ in some sense, i.e., $\\lVert \\mathbf{A}_{k+1} - \\mathbf{A}_{k} \\rVert\\leq\\epsilon$ where $\\lVert\\star\\rVert$ is a [Matrix norm](https://en.wikipedia.org/wiki/Matrix_norm), e.g., the $p = 1$ norm and $\\epsilon$ is a tolerance parameter.\n",
    "\n",
    "Under certain conditions, [QR iteration](https://en.wikipedia.org/wiki/QR_algorithm) will converge to a triangular matrix with the eigenvalues of the original matrix $\\mathbf{A}$ listed on the diagonal. To compute the eigenvectors, we solve the homogenous system of linear algebraic equations:\n",
    "$$\n",
    "\\left(\\mathbf{A} - \\lambda_{\\star}\\mathbf{I} \\right)\\cdot\\mathbf{v}_{\\star} = \\mathbf{0}\n",
    "$$\n",
    "\n",
    "However, before we implement [QR iteration](https://en.wikipedia.org/wiki/QR_algorithm), let's look at how to compute the $\\mathbf{Q}$ and $\\mathbf{R}$ matricies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539e979a-b72c-430d-9b31-10e197f646d8",
   "metadata": {},
   "source": [
    "#### Aside: Classical and Modified Gram-Schmidt\n",
    "The [QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition) can be computed using a variety of approaches, including a handy technique called [the Gram–Schmidt algorithm](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process). Gram-Schmidt computes a set of two or more vectors that are orthogonal (perpendicular) to each other. The vector projection operation is the basic unit operation of [the Gram–Schmidt algorithm](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process). The projection of vector $\\mathbf{v}$ on a nonzero vector $\\mathbf{u}$ is given by: \n",
    "$$\n",
    "P_{\\mathbf{u}}(\\mathbf{v}) = \\frac{\\left<\\mathbf{v},\\mathbf{u}\\right>}{\\left<\\mathbf{u},\\mathbf{u}\\right>}\\cdot\\mathbf{u}\n",
    "$$\n",
    "where $\\left<\\star,\\star\\right>$ denotes [an inner product](https://mathworld.wolfram.com/InnerProduct.html).  Classical Gram-Schmidt allows us to compute a set of orthogonal vectors using the projection operation shown above, i.e., given vectors $\\mathbf{v}_{1},\\mathbf{v}_{2},\\dots,\\mathbf{v}_{k}$ the classical Gram–Schmidt process defines the vectors $\\mathbf{u}_{1},\\mathbf{u}_{2},\\dots,\\mathbf{u}_{k}$ as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{u}_{1} & = \\mathbf{v}_{1} \\\\\n",
    "\\mathbf{u}_{2} & = \\mathbf{v}_{2} - P_{\\mathbf{u}_{1}}(\\mathbf{v}_{2})\\\\\n",
    "\\mathbf{u}_{3} & = \\mathbf{v}_{3} - P_{\\mathbf{u}_{1}}(\\mathbf{v}_{2}) - P_{\\mathbf{u}_{2}}(\\mathbf{v}_{3})\\\\\n",
    "\\vdots & =  \\vdots \\\\\n",
    "\\mathbf{u}_{k} & = \\mathbf{v}_{k} - \\sum_{j=1}^{k-1}P_{\\mathbf{u}_{j}}(\\mathbf{v}_{k})\n",
    "\\end{align*}\n",
    "$$\n",
    "Classical Gram-Schmidt can sometimes produce _almost_ orthogonal vectors because of roundoff error. Let's look at a classic Gram-Schmidt example before we fix this issue and compute our $\\mathbf{Q}$ and $\\mathbf{R}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a16764c-4063-4a89-ac9b-8e549ce594e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(A, Q) = let\n",
    "    A = randn(10,3);\n",
    "    Q = orthogonalize(A, ClassicalGramSchmidtAlgorithm())\n",
    "    A,Q\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06f7ff1-061b-49ea-bcd6-f59e29f14c2e",
   "metadata": {},
   "source": [
    "Once we have $\\mathbf{Q}$, we can compute the $\\mathbf{R}$ factor as:\n",
    "$$\n",
    "\\mathbf{R} = \\mathbf{Q}^{T}\\mathbf{A}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ee1ed29-f25a-455e-bdbf-fbeeeea32e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = transpose(Q)*A;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1cbb584-da5d-47fe-9f45-c4774d593cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×3 Matrix{Float64}:\n",
       " -0.11095     0.0562906  -0.293299\n",
       " -2.42952     0.600019    0.0394207\n",
       "  0.370953   -1.93396     0.510159\n",
       "  1.16786    -1.22952     1.04411\n",
       " -1.76285     0.175377   -0.981568\n",
       " -1.64329     1.11924    -3.0703\n",
       " -0.954255    0.140181   -0.916317\n",
       "  0.0939819  -1.07469     0.621494\n",
       "  0.524992    0.199492   -0.495168\n",
       "  0.86224    -0.491428   -1.06302"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa347b34-d9c4-4652-b15d-6274dc634233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×3 Matrix{Float64}:\n",
       " -0.11095     0.0562906  -0.293299\n",
       " -2.42952     0.600019    0.0394207\n",
       "  0.370953   -1.93396     0.510159\n",
       "  1.16786    -1.22952     1.04411\n",
       " -1.76285     0.175377   -0.981568\n",
       " -1.64329     1.11924    -3.0703\n",
       " -0.954255    0.140181   -0.916317\n",
       "  0.0939819  -1.07469     0.621494\n",
       "  0.524992    0.199492   -0.495168\n",
       "  0.86224    -0.491428   -1.06302"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q*R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d40fed-7a52-4c38-bf05-a2ee956e6042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
