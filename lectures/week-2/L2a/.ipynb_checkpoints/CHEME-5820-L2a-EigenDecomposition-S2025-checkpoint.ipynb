{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d0c28b-498e-4e20-a8ea-8ea5a5aecbac",
   "metadata": {},
   "source": [
    "# Lecture 2a: Eigendecomposition of Data and Systems\n",
    "In this lecture, we will discuss the eigendecomposition of a square matrix and how it can be used to understand data and systems in unsupervised machine learning. There are several key ideas in this lecture:\n",
    "\n",
    "* __Eigendecomposition__ allows us to decompose a matrix into its constituent parts, the [eigenvectors and eigenvalues](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors). These values can help us understand the structure of the data or system represented by the matrix. We'll look at two approaches to estimate the [eigenvalues and eigenvectors](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors) of a matrix.\n",
    "* __Power iteration method__ estimates the _largest_ eigenvalue/eigenvector pair. Given a _diagonalizable_ matrix $\\mathbf{A}$ the power iteration algorithm will produce a number $\\lambda$, which is the greatest (in absolute value) eigenvalue of $\\mathbf{A}$ and a nonzero vector $\\mathbf{v}$ which is a corresponding eigenvector of $\\lambda$ such that $\\mathbf{A}\\mathbf{v} = \\lambda\\cdot\\mathbf{v}$.\n",
    "* __QR factorization__ is another approach to compute the eigendecomposition of the matrix $\\mathbf{A}$. However, unlike power iteration, this approach will give all eigenvalues and eigenvectors of the matrix $\\mathbf{A}$. The QR factorization algorithm relies on the [QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition), which itself relies on the [Gram-Schmidt algorithm](https://en.wikipedia.org/wiki/Gram–Schmidt_process).\n",
    "\n",
    "Lecture notes can be found: [here!](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-2/L2a/docs/Notes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9085c0c-4cbd-4e22-9b47-1127307d413f",
   "metadata": {},
   "source": [
    "## Setup and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. The `Include.jl` file loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc0dc6e0-1af4-4406-af0f-67e920dac536",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde34514-8a96-4197-80e3-8aaa98834ab7",
   "metadata": {},
   "source": [
    "We'll use the coagulation dataset. Let's load this data from disk using [the `MySyntheticDataSet()` function](src/Files.jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afab24e4-49f5-4235-bad3-014cfe722e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MySyntheticDataset() |> d-> d[\"ensemble\"]; "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2e02d2-4996-4279-acd9-ff1c2c99c622",
   "metadata": {},
   "source": [
    "The keys of the dataset dictionary are the `actual` patient indexes. These keys point to `synthetic` patient measurement vectors constructed by building a model of the original data distribution. To explore this data, specify an original patient index (one of the keys of the original dictionary) in the `original_patient_index::Int` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b5d4612-a983-41c4-965a-d47fbe0ed083",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_patient_index = 7; # i ∈ {keys}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ef3440-055c-4a50-b5bd-d4f9eab9f87a",
   "metadata": {},
   "source": [
    "Next, we'll build a data matrix with the `synthetic` measurement vectors for the specified original patient index. We'll store this in the `D::Array{<:Number, 1}` matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0eb22fb3-5846-48a1-b8fe-2330a8d921e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = let\n",
    "\n",
    "    M = dataset[original_patient_index];\n",
    "    number_of_rows = length(M); # number of synthetic patients\n",
    "    number_of_cols = length(M[1]) - 1; # number of measurements (features), first col is the visit number\n",
    "    D = Array{Float64,2}(undef, number_of_rows, number_of_cols);\n",
    "\n",
    "    for i ∈ 0:(number_of_rows - 1)\n",
    "        for j ∈ 1:(number_of_cols)\n",
    "            D[i+1,j] = M[i][j+1];\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # D̂ = copy(D); # no z-scale this data\n",
    "    # for j ∈ 1:number_of_cols\n",
    "    #     sample_vector = D[:,j]; \n",
    "    #     μ = mean(sample_vector);\n",
    "    #     σ = std(sample_vector);\n",
    "\n",
    "    #     for i ∈ 1:number_of_rows\n",
    "    #         D̂[i,j] = (sample_vector[i] - μ)/σ;\n",
    "    #     end\n",
    "    # end\n",
    "    \n",
    "    D\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17d732d-e631-418c-b342-04b61cc08c1c",
   "metadata": {},
   "source": [
    "Finally, we set some constants that we'll use throughout the lecture. See the comment beside the constant value for its meaning, permissible values, units, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79d0f502-456a-4cca-acd2-be6859cd808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_examples = size(D,1); # number of synthetic patients\n",
    "number_of_features = size(D,2); # number of features (measurements)\n",
    "maxiter = 25000; # maximum number of iterations\n",
    "ϵ = 1e-8; # stopping criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a344371-a7f4-4377-aada-441a1aa149d1",
   "metadata": {},
   "source": [
    "## Eigendecomposition\n",
    "Suppose we have a real square matrix $\\mathbf{A}\\in\\mathbb{R}^{m\\times{m}}$ which could be a measurement dataset, e.g., the columns of $\\mathbf{A}$ represent feature \n",
    "vectors $\\mathbf{x}_{1},\\dots,\\mathbf{x}_{m}$ or an incidence array in a graph, etc. Eigenvalue-eigenvector problems involve finding a set of scalar values $\\left\\{\\lambda_{1},\\dots,\\lambda_{m}\\right\\}$ called \n",
    "[eigenvalues](https://mathworld.wolfram.com/Eigenvalue.html) and a set of linearly independent vectors \n",
    "$\\left\\{\\mathbf{v}_{1},\\dots,\\mathbf{v}_{m}\\right\\}$ called [eigenvectors](https://mathworld.wolfram.com/Eigenvector.html) such that:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{A}\\cdot\\mathbf{v}_{j} = \\lambda_{j}\\cdot\\mathbf{v}_{j}\\qquad{j=1,2,\\dots,m}\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\mathbf{v}\\in\\mathbb{R}^{m}$ and $\\lambda\\in\\mathbb{R}$. So, why is this interesting?\n",
    "* Eigenvectors represent fundamental directions of the matrix $\\mathbf{A}$. For the linear transformation defined by a matrix $\\mathbf{A}$, the eigenvectors are the only vectors that do not change direction during the transformation.\n",
    "* Eigenvalues are scale factors for their eigenvector. An eigenvalue is a scalar that indicates how much a corresponding eigenvector is stretched or compressed during a linear transformation represented by the matrix $\\mathbf{A}$.\n",
    "\n",
    "Another interpretation we'll explore later is that eigenvectors represent the most critical directions in the data or system, and the eigenvalues represent the importance of these directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f68694-55fd-4352-82e4-9956f7b48855",
   "metadata": {},
   "source": [
    "## Method 1: Power iteration\n",
    "The [power iteration method](https://en.wikipedia.org/wiki/Power_iteration) is an iterative algorithm to compute the largest eigenvalue and its corresponding eigenvector of a square (real) matrix; we'll consider only real-valued matrices here, but this approach can be used for matrices with complex entries. \n",
    "\n",
    "__Eigenvector__: Suppose we have a real-valued square _diagonalizable_ matrix $\\mathbf{A}\\in\\mathbb{R}^{m\\times{m}}$ whose eigenvalues have the property $|\\lambda_{1}|\\geq|\\lambda_{2}|\\dots\\geq|\\lambda_{m}|$. Then, the eigenvector $\\mathbf{v}_{1}$ which corresponds to the largest eigenvalue $\\lambda_{1}$ can be (iteratively) estimated as:\n",
    "$$\n",
    "\\mathbf{v}_{1}^{(k+1)} = \\frac{\\mathbf{A}\\mathbf{v}_{1}^{(k)}}{\\Vert \\mathbf{A}\\mathbf{v}_{1}^{(k)} \\Vert}\\quad{k=0,1,2\\dots}\n",
    "$$\n",
    "\n",
    "where $\\lVert \\star \\rVert$ denotes the [L2 (Euclidean) norm](https://mathworld.wolfram.com/L2-Norm.html). The [power iteration method](https://en.wikipedia.org/wiki/Power_iteration) converges to a value for the eigenvector as $k\\rightarrow\\infty$ when a few properties are true, namely, $|\\lambda_{1}|/|\\lambda_{2}| < 1$, and we pick an appropriate initial guess for $\\mathbf{v}_{1}$.\n",
    "\n",
    "__Eigenvalue__: Once we have an estimate for the eigenvector $\\hat{\\mathbf{v}}_{1}$, we can estimate the corresponding eigenvalue $\\hat{\\lambda}_{1}$ using [the Rayleigh quotient](https://en.wikipedia.org/wiki/Rayleigh_quotient). We know, from the definition of eigenvalue-eigenvector pairs, that:\n",
    "$$\n",
    "\\mathbf{A}\\hat{\\mathbf{v}}_{1} - \\hat{\\lambda}_{1}\\hat{\\mathbf{v}}_{1}\\simeq{0}\n",
    "$$\n",
    "To solve this for the eigenvalue $\\hat{\\lambda}_{1}$, we can multiply through by the transpose of the eigenvector and solve for the eigenvalue:\n",
    "$$\n",
    "\\hat{\\lambda}_{1} \\simeq \\frac{\\hat{\\mathbf{v}}_{1}^{T}\\mathbf{A}\\hat{\\mathbf{v}}_{1}}{\\hat{\\mathbf{v}}_{1}^{T}\\hat{\\mathbf{v}}_{1}}\n",
    "$$\n",
    "Note that we have used the $\\simeq$ symbol as this expression will give the true eigenvalue only when we have the true eigenvector. In our case, we have an approximation of the eigenvector, which could be a good or poor approximation depending on how many iterations we take.\n",
    "\n",
    "__Algorithm__\n",
    "* __Initialization__. We begin (iteration $k=0$) with an initial (random) guess of the eigenvector $\\mathbf{v}_{1}^{(0)}$, the maximum number of iterations we are willing to take `maxiter,` and a tolerance parameter $\\epsilon>0$.  \n",
    "* __Update__: Next, we repeatedly multiply the $\\mathbf{v}^{\\star}_{1}$ vector by the matrix $\\mathbf{A}$ and normalize the result by $\\Vert\\mathbf{A}\\mathbf{v}^{\\star}_{1}\\Vert$. This iterative approach capitalizes on the property that the dominant eigenvalue will exert the most influence on the vector $\\mathbf{v}$ over successive iterations, allowing it to converge towards the eigenvector associated with the largest eigenvalue.\n",
    "* __Stopping__: We stop the iteration procedure after `maxiter` number of iterations is reached or when the difference between successive iterations is _small_ in some sense, i.e., $\\lVert \\mathbf{v}_{1}^{(k)} - \\mathbf{v}_{1}^{(k-1)} \\rVert\\leq\\epsilon$ where $\\lVert\\star\\rVert$ is [some vector norm](https://mathworld.wolfram.com/VectorNorm.html). In practice, we'll use both stopping criteria (and the L2 norm) to guard against an infinite loop.\n",
    "\n",
    "While simple to implement, the [power iteration method](https://en.wikipedia.org/wiki/Power_iteration) may exhibit slow convergence, mainly when the largest eigenvalue is close in magnitude to other eigenvalues. \n",
    "Check out a [power iteration pseudo-code here!](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-1/L1c/figs/pcode-kmeans.pdf)\n",
    "\n",
    "Additional references:\n",
    "* https://www.cs.cornell.edu/~bindel/class/cs6210-f16/lec/2016-10-17.pdf\n",
    "* https://blogs.sas.com/content/iml/2012/05/09/the-power-method.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b33aa3-9663-4583-92ed-a51f2684ea49",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "We've implemented the [power iteration method](https://en.wikipedia.org/wiki/Power_iteration) in the [`poweriteration(...)` function](src/Compute.jl). \n",
    "* The [`power iteration (...)` function](src/Compute.jl) takes the square matrix $\\mathbf{A}$, an initial guess for the eigenvector $\\mathbf{v}^{(0)}_{1}$ and (optional) keyword parameters controlling the stopping criteria as arguments. The function returns a tuple holding the estimated eigenvector $\\hat{\\mathbf{v}}_{1}$ and eigenvalue $\\hat{\\lambda}_{1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9e86e281-4205-4071-bf8e-16145097eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "(v̂,λ̂) = let\n",
    "\n",
    "    A = transpose(D)*D; # build a square matrix from the data\n",
    "    n = size(A,1); # how many rows (cols) do we have?\n",
    "    vₒ = randn(n); # initial random guess\n",
    "\n",
    "    # call the poweriteration function\n",
    "    (v, λ) = poweriteration(A, vₒ, maxiter = maxiter, ϵ = ϵ);\n",
    "\n",
    "    # return -\n",
    "    (v,λ)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e77bf0-02a0-484d-863f-96aeedc3045d",
   "metadata": {},
   "source": [
    "### Test\n",
    "To test our implementation, let's compare the values of $(\\hat{\\lambda}_{1}, \\hat{\\mathbf{v}}_{1})$ that we estimated versus those computed [using the `eigen(...)` function exported by the LinearAlgebra.jl package](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigen). The built-in [`eigen(...)` function](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigen) takes the matrix $\\mathbf{A}$ (and some additional optional arguments) and returns [an `Eigen` factorization object](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.Eigen) holding the eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c91e29b8-b4ea-416b-9d7c-612adc22b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    A = transpose(D)*D; # build a square matrix from the data\n",
    "    F = eigen(A);\n",
    "\n",
    "    λ = maximum(F.values); # get the max eigenvalue (sorted, this should be the last element)\n",
    "    v = F.vectors[:,end]; # eigenvectors are sorted - the last column is v₁\n",
    "\n",
    "    # tests\n",
    "    @assert λ ≈ λ̂ # eigenvalues match?\n",
    "    @assert abs.(v) ≈ abs.(v̂) # eigenvectors match? (why abs?)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4080a7-b124-4045-801d-121835f0042f",
   "metadata": {},
   "source": [
    "## Method 2: QR factorization and Gram-Schmidt\n",
    "Fill me in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7996efd3-fe2a-4aa4-a9a3-f7469c0d1338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Eigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\n",
       "values:\n",
       "32-element Vector{Float64}:\n",
       "      0.7965972275689526\n",
       "      0.8701429641944097\n",
       "      2.039551836582301\n",
       "      2.7904185818636726\n",
       "      4.190406985710426\n",
       "     10.500815662637573\n",
       "     13.066580433098135\n",
       "     45.2197229642944\n",
       "     56.38952446979662\n",
       "     73.71270386779292\n",
       "    108.95065243994259\n",
       "    196.56312777062917\n",
       "    218.30174625892894\n",
       "      ⋮\n",
       "   8012.657170514377\n",
       "  10887.201683745288\n",
       "  17516.934946637335\n",
       "  31896.468087663383\n",
       "  51257.711158272316\n",
       "  70895.1030640178\n",
       " 515727.08641797875\n",
       "      2.0513055222368066e6\n",
       "      3.0972800898292297e6\n",
       "      4.241929776681231e6\n",
       "      1.4358778322967645e7\n",
       "      8.158508857877813e9\n",
       "vectors:\n",
       "32×32 Matrix{Float64}:\n",
       " -0.00178058    0.0385867     0.098397     …   0.000158009  -4.03042e-5\n",
       "  0.0447398    -0.0196024    -0.00142773       0.000314894  -0.000145721\n",
       "  0.000297513  -0.000988625  -0.00161957       0.177651     -0.148088\n",
       "  0.0390617    -0.00581845   -0.0149327        0.00344472   -0.00220036\n",
       " -0.0483561    -0.0425786     0.0846021        0.00116242   -0.00126853\n",
       " -0.654779     -0.693934     -0.0317681    …   0.000133714  -9.00124e-5\n",
       "  0.00669937   -0.00728502   -0.00514089       0.0178069    -0.0125568\n",
       " -0.000897078   0.00603387    0.000488631      0.0233065    -0.0170449\n",
       " -0.0351253     0.0308533     0.058622         0.00190338   -0.00141845\n",
       "  0.0289344    -0.0123942    -0.0145108        0.00210842   -0.00127319\n",
       "  0.000504187   0.000300705  -0.000850057  …   0.697348     -0.400578\n",
       "  0.00766803   -0.00492551   -0.00871126       0.014776     -0.0102943\n",
       " -0.50919       0.421741      0.715493         0.000349377  -0.000169274\n",
       "  ⋮                                        ⋱   ⋮            \n",
       "  0.0454987     0.0289058    -0.03919      …   0.000372033  -0.000166976\n",
       " -0.00303959   -0.00232143    0.000935356      0.000271497  -0.000672178\n",
       " -0.000380474  -0.000198422   0.000541965      0.493514     -0.148408\n",
       " -0.00259826   -0.00013085    0.00122982       0.0373216    -0.0199868\n",
       "  0.00423518   -0.00909419    0.00350468       0.00386971   -0.00428258\n",
       " -0.0961526    -0.231146      0.00993449   …   0.00102736   -0.000590437\n",
       " -0.0734577    -0.127896     -0.111146        -5.73387e-5   -0.0020161\n",
       " -0.0197712    -0.0161156    -0.0356447        0.00393233   -0.00507881\n",
       "  0.0496922    -0.0690055     0.00284186       0.00792667   -0.00412587\n",
       " -0.00932654    0.0251938     0.0167788        0.0168585    -0.011013\n",
       " -0.0368212     0.049168      0.00462657   …   0.0149116    -0.00643448\n",
       "  0.000751084   0.00190803    0.000835873      0.0122884    -0.201028"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = transpose(D)*D; # build a square matrix from the data\n",
    "eigen(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b5b354-7a9d-4981-a356-a49896da1bff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
