{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9edea839-d6ec-4e40-802a-4abd1e2ae4ab",
   "metadata": {},
   "source": [
    "# L3a: Linear regression models for prediction and classification tasks\n",
    "This lecture explores linear regression models for prediction and classification. Linear regression predicts continuous target variables using one or more features (prediction) and classifies objects based on features (classification). These models are easy to implement and interpret, serving as a foundation for more complex predictive analytics algorithms.\n",
    "\n",
    "There are several key ideas in this lecture:\n",
    "\n",
    "* __Linear regression__ is a statistical method for modeling the relationship between a dependent variable (target) and one or more independent variables (features) by fitting a linear equation to observed data. It provides a simple way to predict outcomes and understand relationships between variables.\n",
    "* __Continuous variable__ prediction tasks: In machine learning, linear regression models are commonly employed for continuous variable prediction tasks. These models enable the estimation of numerical outcomes based on the (non)linear relationships identified between input features and the target variable.\n",
    "* __Classification tasks__: While linear regression is primarily designed to predict continuous outcomes, it can also be adapted for classification tasks by combining the linear regression model with a classification function that transforms the continuous target variable predictions into discrete classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f92d34-8caa-42d5-a09a-2a0cec96285a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Setup, Data and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. The `Include.jl` file loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf6f0689-34c4-4758-8427-e7de25160311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling IJuliaExt [2f4121a4-3b3a-5ce6-9c5e-1f2673ce168a] (cache misses: wrong dep version loaded (20))\n"
     ]
    }
   ],
   "source": [
    "include(\"Include.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f16d67b-ec02-469e-9943-c95bec414fb0",
   "metadata": {},
   "source": [
    "## Linear regression models for continuous prediction tasks\n",
    "Suppose there exists a dataset $\\mathcal{D} = \\left\\{\\mathbf{x}_{i},y_{i}\\right\\}_{i=1}^{n}$ with $n$-training (labeled) examples, where $\\mathbf{x}_{i}\\in\\mathbb{R}^{p}$ is a $p$-vector of features (independent variables, typically real but potentially complex as well) and $y_{i}\\in\\mathbb{R}$ denotes a scalar response variable (dependent variable). Then, a $\\texttt{linear regression model}$ for the dataset $\\mathcal{D}$ takes the form:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "y_{i} = \\mathbf{x}_{i}^{T}\\cdot\\mathbf{\\beta} + \\epsilon_{i}\\qquad{i=1,2,\\dots,n}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $\\mathbf{\\beta}\\in\\mathbb{R}^{p}$ is a $p\\times{1}$ vector of unknown model parameters, and $\\epsilon_{i}\\in\\mathbb{R}$ is the unobserved random error for response $i$. The linear regression model in matrix-vector form is given by:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\mathbf{y} = \\mathbf{X}\\cdot\\mathbf{\\beta} + \\mathbf{\\epsilon}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $\\mathbf{X}$ is an $n\\times{p}$ matrix with the features $\\mathbf{x}_{i}^{T}$ on the rows, \n",
    "the response vector $\\mathbf{y}$ is an $n\\times{1}$ vector with entries $y_{i}$, \n",
    "and the error vector $\\mathbf{\\epsilon}$ is an $n\\times{1}$ vector with entries $\\epsilon_{i}$. The challenge of linear regression is to estimate the unknown parameters $\\mathbf{\\beta}$ from the dataset $\\mathcal{D}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7ab630-1bc6-4c17-a40d-5ec591f41c68",
   "metadata": {},
   "source": [
    "### Case I: Overdetermined data matrix\n",
    "Let the data matrix $\\mathbf{X}$ be $\\texttt{overdetermined}$, i.e., $n > p$ (more rows than columns), and the error vector $\\mathbf{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\cdot\\mathbf{I})$.\n",
    "Then, the [ordinary least squares](https://en.wikipedia.org/wiki/Ordinary_least_squares) estimate of the unknown parameters $\\mathbf{\\beta}$ will $\\textit{minimize}$ the sum of squared errors between model estimates and observed values:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\hat{\\mathbf{\\beta}} = \\arg\\min_{\\mathbf{\\beta}} ||~\\mathbf{y} - \\mathbf{X}\\cdot\\mathbf{\\beta}~||^{2}_{2}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $||\\star||^{2}_{2}$ is the square of the [`p = 2` vector norm](https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm), and $\\hat{\\mathbf{\\beta}}$ denotes the estimated parameter vector.  The parameters $\\hat{\\mathbf{\\beta}}$ that minimize the $||\\star||^{2}_{2}$ loss for an overdetermined data matrix $\\mathbf{X}$ are given by:\n",
    "\\begin{equation*}\n",
    "\\hat{\\mathbf{\\beta}} = \\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{T}\\mathbf{y} - \\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{T}\\mathbf{\\epsilon}\n",
    "\\end{equation*}\n",
    "The matrix $\\mathbf{X}^{T}\\mathbf{X}$ is the $\\texttt{normal matrix}$, while $\\mathbf{X}^{T}\\mathbf{y}$ is the $\\texttt{moment vector}$. The inverse $\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}$ must exist to obtain the estimated model parameter vectors $\\hat{\\mathbf{\\beta}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8135be5-ccda-49ae-8973-a6c8d5056f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc3999d-02f7-4ad2-9878-02626414cc6a",
   "metadata": {},
   "source": [
    "### Case II: Underdetermind data matrix\n",
    "Assume the data matrix $\\mathbf{X}$ is $\\texttt{underdetermined}$, i.e., $n < p$ (more columns than rows), and \n",
    "the error vector $\\mathbf{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\cdot\\mathbf{I})$.\n",
    "Then, an [ordinary least squares](https://en.wikipedia.org/wiki/Ordinary_least_squares) estimate of the unknown parameters is the $\\textit{smallest}$ parameter vector $\\beta$ that satisfies the original equations:\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{minimize}~& & ||\\,\\mathbf{\\beta}\\,|| \\\\\n",
    "\\text{subject to} & & \\mathbf{X}\\cdot\\mathbf{\\beta} = \\mathbf{y}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "The least-norm problem has an analytical estimate for the unknown parameter vector $\\hat{\\mathbf{\\beta}}$ given by:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\hat{\\mathbf{\\beta}} =\\mathbf{X}^{T}\\left(\\mathbf{X}\\mathbf{X}^{T}\\right)^{-1}\\cdot\\mathbf{y} - \\mathbf{X}^{T}\\left(\\mathbf{X}\\mathbf{X}^{T}\\right)^{-1}\\cdot\\mathbf{\\epsilon}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where inverse $\\left(\\mathbf{X}\\mathbf{X}^{T}\\right)^{-1}$ must exist to obtain the estimated model parameter vectors $\\hat{\\mathbf{\\beta}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "27010586-d186-4a2e-835e-0bdf3637810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7610f92-0b13-4cd3-bee5-a640088e8953",
   "metadata": {},
   "source": [
    "### Case III: Regularized linear regression models\n",
    "Regularized linear regression models incorporate penalty terms to constrain the size of the coefficient estimates, thereby reducing overfitting and enhancing the model's generalizability to new data. Consider an overdetermined data matrix $\\mathbf{X}\\in\\mathbb{R}^{n\\times{p}}$, i.e., the case where $n>p$ (more examples than unknown parameters).\n",
    "\n",
    "A regularized least squares estimate of the unknown parameters $\\mathbf{\\beta}$ for an _overdetermined_ system will _minimize_ a loss (objective) function of the form:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\hat{\\mathbf{\\beta}} = \\arg\\min_{\\mathbf{\\beta}} ||~\\mathbf{y} - \\mathbf{X}\\cdot\\mathbf{\\beta}~||^{2}_{2} + \\lambda\\cdot||~\\mathbf{\\beta}~||^{2}_{2}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $||\\star||^{2}_{2}$ is the square of the [`p = 2` vector norm](https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm), $\\lambda\\geq{0}$ denotes a regularization parameter, and $\\hat{\\mathbf{\\beta}}$ denotes the estimated parameter vector. \n",
    "The parameters $\\hat{\\mathbf{\\beta}}$ that minimize the $||\\star||^{2}_{2}$ loss plus penalty for overdetermined data matrix $\\mathbf{X}$ are given by:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\hat{\\mathbf{\\beta}}_{\\lambda} = \\left(\\mathbf{X}^{T}\\mathbf{X}+\\lambda\\cdot\\mathbf{I}\\right)^{-1}\\mathbf{X}^{T}\\mathbf{y} - \\left(\\mathbf{X}^{T}\\mathbf{X}+\\lambda\\cdot\\mathbf{I}\\right)^{-1}\\mathbf{X}^{T}\\mathbf{\\epsilon}\n",
    "\\end{equation*}\n",
    "$$\n",
    "The matrix $\\mathbf{X}^{T}\\mathbf{X}+\\lambda\\cdot\\mathbf{I}$ is the $\\texttt{regularized normal matrix}$, while $\\mathbf{X}^{T}\\mathbf{y}$ is the $\\texttt{moment vector}$. The inverse $\\left(\\mathbf{X}^{T}\\mathbf{X}+\\lambda\\cdot\\mathbf{I}\\right)^{-1}$ must exist to obtain the estimated parameter vector $\\hat{\\mathbf{\\beta}}_{\\lambda}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "20c2e58a-c214-4eba-a947-45796a96ed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8a4a0b-d61b-4948-a94e-12ceac3a6a71",
   "metadata": {},
   "source": [
    "## Linear regression models for classification tasks\n",
    "Linear regression can be adapted for classification tasks by interpreting its continuous output as probabilities and applying a threshold to categorize predictions into discrete classes. \n",
    "* This approach allows for a rudimentary classification mechanism. However, it can lead to unreliable results due to the inherent limitations of linear regression in defining clear class boundaries. Thus, it is less suitable than dedicated classification algorithms like [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression#).\n",
    "* Linear regression plays a crucial role in [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression#) by serving as the foundational model that estimates the linear combination of input features, which is then transformed using the logistic function to produce probabilities for binary classification tasks.\n",
    "\n",
    "### Perceptron\n",
    "Suppose we take the (scalar) output of a linear regression model $y_{i}\\in\\mathbb{R}$ and then transform it using some function $\\sigma(\\star)$ to a discrete set of values representing categories, e.g., $\\sigma:\\mathbb{R}\\rightarrow\\{-1,1\\}$ in the binary classification case. This idea underlies [the Perceptron (Rosenblatt, 1957)](https://en.wikipedia.org/wiki/Perceptron). \n",
    "\n",
    "* The Perceptron learns a linear decision boundary between _two_ classes of objects (binary classification) by repeatedly processing data until it makes no more than a specified number of mistakes. Suppose there exists a data set\n",
    "$\\mathcal{D} = \\left\\{(\\mathbf{x}_{1},y_{1}),\\dotsc,(\\mathbf{x}_{m},y_{m})\\right\\}$ with $m$ examples, where each example has been labeled by an expert, i.e., a human to be in category $\\hat{y}_{i}\\in\\{-1,1\\}$, given the feature vector $\\mathbf{x}_{i}\\in\\mathbb{R}^{n}$. \n",
    "\n",
    "The Perceptron computes label that $\\hat{y}$ for feature vector $\\mathbf{x}$ as:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\hat{y} = \\text{sign}\\left(\\mathbf{w}^{T}\\cdot\\mathbf{x}\\right)\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $\\mathbf{w}=\\left(w_{1},\\dots,w_{N}, b\\right)$ is a vector of weights $w$ and a bias $b$, \n",
    "$\\mathbf{x}=\\left(x_{1},\\dots,x_{n}, 1\\right)$ is a feature vector,\n",
    "and $\\text{sign}(z)$ is the sign function:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\text{sign}(z) = \n",
    "    \\begin{cases}\n",
    "        1 & \\text{if}~z\\geq{0}\\\\\n",
    "        -1 & \\text{if}~z<0\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "$$\n",
    "If data set $\\mathcal{D}$ is linearly separable, the Perceptron will find a separating hyperplane in a finite number of passes through $\\mathcal{D}$. However, if the data set $\\mathcal{D}$ is not linearly separable, the Perceptron may not converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895ae0c4-f06e-443b-932f-25d2528623cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
